
```{r 'check_ps', include=FALSE}

user.name = 'ENTER A USER NAME HERE'
```



# Militarization of US police

Author: Jonas Grüneberg

Welcome! I am glad you found your way to this interactive analysis in R. This problem set is part of my master thesis at Ulm University and is based on the paper **"Police Officer on the Frontline or a Soldier? The Effect of Police Militarization on Crime"** written by Vincenzo Bove and Evelina Gavrilova. The study investigates the causal effects of an increase in militarization of US local police forces on their effectiveness in preventing and solving crime. Both the article and the data can be found [here](https://www.aeaweb.org/articles?id=10.1257/pol.20150478).

**Note:** Please open all links in a new tab by right clicking and selecting "Open link in new tab". This will prevent you from losing any progress you may have made in the problem set. 

The problem set is published here:

* `GitHub`: 
* `ShinyApps`: 

Recently, the militarization of local police forces in the United States has become a subject of increasing public and academic concern. Central to this development is the 1033 Program, which enables the transfer of surplus military equipment from the Department of Defense to civilian law enforcement agencies. While proponents argue that such equipment enhances officer safety and preparedness (Bove & Gavrilova, 2017), critics fear it may contribute to excessive use of force, community alienation, and the erosion of democratic policing norms (Balko, 2013; Kraska, 2007).

Empirical research on the effects of militarized policing has produced mixed results. Some studies suggest that access to military equipment does not reduce crime and may worsen police–community relations (Mummolo, 2018), while others point to context-dependent deterrent effects (Delehanty et al., 2017). However, causal identification remains a major challenge, as aid allocation is endogenous to local crime patterns and agency characteristics.

With this problem set, we aim to evaluate whether and how military aid provided through the 1033 Program has contributed to reducing crime. To that end, we systematically reconstruct the researchers’ empirical approach, explaining each methodological step in detail and gradually replicating the key results of their analysis.

</br>
  
## Exercise Content

1. Motivation

  1.1 Why Police Militarization Matters: Motivating the Research Question

  1.2 Militarization and Institutional Change: Understanding the 1033 Program

2. Data Structure and Exploration

  2.1 Getting Started with the Datasets: Understanding Structure and Content

  2.2 Exploring the Dataset in Depth: Understanding Key Metrics and Distributions

  2.3 Graphical Analysis: Understanding Trends and Patterns

3. Identification and Causal Inference

  3.1 Military Aid and Crime: A Fixed Effects Approach

  3.2 Standard Errors and Clustering: Ensuring Robust Inference

  3.3 Instrumental Variables: Introducing the Method through Simulation

  3.4 Instrumental Variables and Military Aid: Causal Estimates in Panel Data

4. Mechanisms and Broader Effects

  4.1 Crime Types: Disaggregating the Impact of Military Aid

  4.2 Arrest Rates and the Mechanisms of Deterrence

  4.3 Beyond Crime: Organizational and Behavioral Effects of Militarization
  
  4.4 Supply Categories: Effectiveness of Military Aid

5. Conclusion

6. References

</br>

## Overview

We begin by exploring the broader debate surrounding the militarization of U.S. police forces, with particular attention to the central policy instrument at the heart of this discussion: the 1033 Program. This step sets the stage for understanding the institutional and political relevance of the topic. Next, we will become familiar with the datasets used throughout the problem set. These provide detailed information on military aid, crime statistics, and law enforcement characteristics at the county and agency levels. The core of the problem set consists of a step-by-step development of an empirical strategy to estimate the causal effect of military aid on crime. We begin with simple regression models and incrementally refine them by introducing control variables, fixed effects, and cluster-robust standard errors. The centerpiece of our identification strategy is the instrumental variable (IV) approach, which allows us to address endogeneity concerns and strengthen causal inference. In the final analytical section, we examine the mechanisms through which military aid affects crime by analyzing arrest rates and other law enforcement outcomes. This process helps us distinguish between deterrent effects and changes in police effectiveness. The last section concludes by summarizing the main findings and offering an outlook on future research directions.

## How to Work on the Problem Set

When solving the exercises, you do not have to follow the structure of the problem set strictly. Nevertheless, I recommend this approach since it is organized like a tour through the entire study. However, within the exercises you still have to follow the order of the tasks. The essence of the problem set are the code chunks, where you will encounter three types:
  
* empty code chunk where you have to find the solution completely by yourself;
* code chunks with gaps where you should complete the existing code by replacing `___`;
* ready to run code chunks where you just `check` the chunk, the entire code is already given;

In case you want to work on a code chunk press `edit` first and in case you need some advice, press the `hint` button. However, it is possible to show the correct code by the  `solution` button. If you want to execute the code without checking it, press simply `run`. Finally, to verify and complete the task, click on `check`.

In addition to code blocks, there are also quizzes you can work on. In some cases, you can simply guess the correct answer, while other questions will help you to gain a deeper understanding and test your knowledge of the topics covered.

As soon as you have finished a (sub-)exercise, click on `Go to next exercise...` to continue.

<br/>
  
## Exercise 1 -- Motivation
  
The first section introduces the topic of police militarization and outlines its significance as a subject of academic and policy-oriented research. It serves to motivate the empirical investigation carried out in the remainder of the problem set by highlighting why the increasing use of military equipment by local police departments deserves critical attention.

To that end, we first discuss the broader relevance of studying police militarization, both in terms of public safety and democratic accountability. We then examine the institutional background and operational mechanisms of the so-called 1033 Program, which facilitates the transfer of surplus military equipment from the U.S. Department of Defense to local law enforcement agencies.

<br/>
  
### Structure
  
1.1 — Why Police Militarization Matters: Motivating the Research Question

1.2 — Militarization and Institutional Change: Understanding the 1033 Program

<br/>


## Exercise 1.1 -- Why Police Militarization Matters: Motivating the Research Question

The militarization of police forces in the United States has become one of the most controversial and widely discussed topics in modern law enforcement and public policy (Delehanty et al., 2017; Kraska, 2007). In recent decades, many local police departments have begun to use military-grade equipment and tactics. This development accelerated with the expansion of the 1033 Program, which allows surplus military equipment to be transferred to law enforcement agencies across the country (Mummolo, 2018). The rise of militarized policing has drawn growing concern from policymakers, researchers, and the public—especially following high-profile incidents in which heavily armed police units were deployed during protests or routine operations (Delehanty et al., 2017; Radil & Dehlin, 2020). These events have sparked debates about the role of militarization in civil society and its broader consequences for democratic policing.

The widespread use of military equipment by police raises important questions about both its effectiveness and its broader impact on society. One key issue is whether giving police access to military-grade weapons, vehicles, and tactical gear actually helps reduce crime—or whether it instead leads to more violence and damaged relationships between police and the communities they serve (Bove & Gavrilova, 2017). But the debate goes beyond crime statistics. Police militarization also affects civil liberties, public trust, and the overall role of law enforcement in a democratic society (Kraska, 2007). This problem set builds on a rigorous research study that uses advanced econometric methods to examine the causal effects of police militarization (Mummolo, 2018). The findings provide valuable insights into a topic that intersects public safety, civil rights, and government accountability (Welch, 2017).

This research is not only important for academics—it also has real-world relevance for policymakers. Understanding how police militarization affects crime, communities, and public trust is essential for making informed decisions about how resources are allocated, which law enforcement strategies are pursued, and what kind of public safety policies are implemented (Bell, 2017). Striking the right balance between effective policing and the protection of civil liberties is a complex challenge. Policymakers must ask whether military aid actually improves public safety or whether it unintentionally heightens tensions between law enforcement and the communities they are meant to serve (Delehanty et al., 2017). In addition, the findings have far-reaching implications for current debates around police reform, community policing, and the broader question of what role law enforcement should play in American society (Balko, 2013).

This topic also reflects broader societal concerns about the growing militarization of public institutions and the possible erosion of democratic values (Radil & Dehlin, 2020). The increasing presence of militarized police units in civilian contexts has raised concerns that the use of force is becoming normalized, even in situations that might be better addressed through de-escalation or community-based approaches (Kraska, 2007). Critics warn that using military-grade equipment and tactics in everyday policing can create an “us vs. them” mentality, weakening the trust and cooperation between police and the communities they are supposed to protect (Delehanty et al., 2017). Supporters, however, argue that militarized equipment improves officer safety and can act as a deterrent to crime, particularly in dangerous or high-risk situations (Bove & Gavrilova, 2017). This ongoing debate points to the need for empirical research to determine the actual effects of police militarization—both on public safety and on the health of civil society (Mummolo, 2018; Welch, 2017).

By working through this problem set, you will learn how econometric methods can help uncover causal relationships in complex policy questions. In particular, you will gain hands-on experience with fixed effects and instrumental variable regressions—two key techniques for separating the true effects of police militarization from other influencing factors (Angrist & Pischke, 2009). This exercise encourages you to think critically about how reliable empirical findings are, to examine the assumptions behind causal inference, and to consider what the results mean for real-world policy.


Quiz: What will you analyze in this problem set?

(1) The cost-efficiency of military equipment in law enforcement.
(2) The effectiveness of police training programs in handling military-grade equipment.
(3) The empirical effects of the police militarization on policing outcomes.
(4) The comparison of military equipment usage in the U.S. versus other countries.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("problemset_focus")
```

<br/>

More than just a technical exercise, this problem set offers a chance to better understand one of the most urgent and debated issues in the United States today. Whether your interests lie in criminology, public policy, or econometrics, the skills and insights you gain here will help you engage more thoughtfully in the ongoing discussion about the role of militarization in policing and its broader effects on society.

<br/>

## Exercise 1.2 -- Militarization and Institutional Change: Understanding the 1033 Program

At the heart of the militarization debate is the 1033 Program, a federal initiative that has significantly transformed the landscape of American law enforcement. This program facilitates the transfer of surplus military equipment from the Department of Defense to local, state, and federal law enforcement agencies across the United States. Established under the National Defense Authorization Act of 1997, the program was initially designed to enhance the operational capabilities of police forces, particularly in the context of the "War on Drugs" and the post-9/11 focus on counterterrorism (Defense Logistics Agency, 2020).

Through the 1033 Program, law enforcement agencies have received a vast array of military equipment, including armored vehicles, assault rifles, grenade launchers, night-vision goggles, and surveillance technology (ACLU, 2014). The program operates on the principle that providing police with advanced military tools would improve their effectiveness in handling high-risk situations, such as drug raids, hostage rescues, and terrorist threats. Since its inception, over $7.6 billion worth of military equipment has been distributed to more than 8,800 law enforcement agencies nationwide (Defense Logistics Agency, 2020).

Proponents of the 1033 Program argue that the transfer of military equipment enhances police preparedness and officer safety, especially in rural areas with limited resources. They claim that access to military-grade tools allows law enforcement to respond more effectively to emergencies, natural disasters, and mass shootings. For example, during incidents involving active shooters or terrorist threats, armored vehicles and tactical gear can provide critical protection for officers and civilians alike. Supporters also highlight the cost-effectiveness of the program, as it allows local agencies to acquire expensive equipment at little to no cost, thereby stretching tight municipal budgets.

However, the program has not been without its critics. Opponents argue that the 1033 Program contributes to the militarization of police culture, fostering an "us vs. them" mentality that can erode public trust and escalate conflicts between law enforcement and the communities they serve (Balko, 2013; Kraska, 2007). The increased presence of militarized police units in routine law enforcement activities—such as traffic stops, warrant executions, and public protests—has sparked widespread concern about the erosion of civil liberties and the normalization of excessive force.

The use of military equipment in non-combat situations, according to critics, blurs the distinction between police officers and soldiers, thereby undermining the principle that law enforcement should act as public protectors rather than enforcers of state power. This shift in policing tactics is believed to affect marginalized communities disproportionately, particularly communities of color, who are more likely to be subjected to aggressive policing practices. Furthermore, studies have suggested that militarized policing does not significantly reduce crime rates and may, in fact, contribute to increased tensions and violent encounters between police and civilians (Mummolo, 2018).


Quiz: What is a major criticism of the 1033 Program?

(1) It fails to provide enough military equipment to law enforcement.
(2) It fosters an adversarial relationship between police and communities, potentially escalating violence.
(3) It does not adequately train officers in using military equipment.
(4) It is too costly for local law enforcement agencies.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("criticism_1033_program")
```

<br/>

The 1033 Program came under intense national scrutiny following highly publicized incidents of police overreach. A pivotal moment was the response to protests in Ferguson, Missouri, in 2014 after the police killing of Michael Brown. The sight of officers in military gear confronting demonstrators drew widespread condemnation. A subsequent Department of Justice investigation revealed patterns of excessive force and systemic racial bias exacerbated by the militarized posture of local police (Department of Justice, 2015).

These events ignited a national conversation about the appropriateness and consequences of equipping local police with military tools. In response, the Obama administration placed restrictions on certain categories of equipment—such as tracked vehicles and high-caliber firearms. However, many of these restrictions were reversed under the Trump administration in 2017, reigniting debate about the scope and legitimacy of police militarization.

Understanding the historical, political, and operational background of the 1033 Program is essential for engaging critically with the empirical analyses in this problem set. The policy context not only informs the variables and research questions we investigate but also helps frame the broader implications of our findings. By exploring the origins, intended purposes, and unintended consequences of the program, you will be better equipped to interpret quantitative results in light of institutional practices and societal debates. In short, this background sets the stage for a nuanced and methodologically grounded examination of police militarization in the United States.

<br/>


## Exercise 2 -- Data Structure and Exploration

In the first exercise, we explored the broader context and motivation of the research, gaining insight into the historical and legal foundations of the 1033 Program and how the transfer of military equipment to local law enforcement agencies is operationalized.

For the empirical analysis, we rely on three datasets:

* The primary dataset is *militarization*, which contains county-level data on military equipment transfers conducted through the 1033 Program between 2005 and 2012. It also includes demographic characteristics and crime rates for six different types of offenses.

* The second dataset, *police*, provides agency-level data and includes many of the same variables as the militarization dataset, supplemented with additional information on the characteristics of law enforcement agencies.  This collection of data allows for more fine-grained analyses of police behavior and institutional effects.

* Finally, the *categories* dataset offers a more detailed classification of transferred equipment, going beyond the four broad equipment groups used in the other two datasets. It is particularly useful for disaggregated analyses of the nature and intensity of militarization.

In this section, we will focus on understanding how the data is structured and organized. This foundational step will help prepare the datasets for the regression analyses and empirical strategies used in the following exercises.


### Structure

2.1 — Getting Started with the Datasets: Understanding Structure and Content

2.2 — Exploring the Dataset in Depth: Understanding Key Metrics and Distributions

2.3 — Graphical Analysis: Understanding Trends and Patterns

<br/>

## Exercise 2.1 -- Getting Started with the Datasets: Understanding Structure and Content

Before conducting any empirical analysis, it is necessary to gain a solid understanding of the structure and content of the datasets involved. This initial exploration lays the groundwork for meaningful insights by helping you identify the scope, granularity, and quality of the available information. In this exercise, we begin by introducing the datasets used in this project, with a primary focus on the *militarization* dataset, which closely resembles the structure of the *police* dataset. The third dataset, called *categories*, serves a complementary purpose by providing detailed classifications for equipment types; however, it plays only a secondary role in our main analysis.

### Loading the Dataset

To facilitate your work, we use curated versions of the datasets, in particular *militarization.RDS*, which has been streamlined to include only variables relevant to our study. This version also includes precomputed values created by the authors and has been cleaned of missing observations (NAs) to ensure data integrity. These files are available in the *data* folder of the problem set and can be easily loaded into R.

To load the dataset, we use the readRDS() function in R. This function reads R-specific data files (.RDS format) and restores them as R objects. For those unfamiliar with readRDS(), a brief guide is provided in the info box below.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("How to Load and Save Data with readRDS()")
```

<br/>

**Task:** Read in the *militarization.RDS* file from the *data* folder and assign it to the variable named `mil`.

```{r "6_3"}
# Enter your code here.
```

### Exploring the Dataset: A First Look

With the dataset loaded, our next step is to familiarize ourselves with its structure. R provides the `head()` function, which displays the first six rows of the dataset. This initial exploration helps us understand the data's format and the variables included.

**Task:** Use the `head()` function to preview the first six rows of the `mil` dataset.

```{r "6_4"}
# Enter your code here.
```

The preview reveals the first six rows of our dataset, which includes 43 columns. Each row represents a unique **county-year** observation, providing information on the transfer of military equipment, crime rates, and various demographic characteristics of the county.

### Overview of Key Dataset Components

#### 1. Military Aid

A central component of the dataset is the record of **military equipment transferred** to local law enforcement agencies through the **1033 Program**. The equipment is categorized into four main groups:

* **Weapons**: Firearms and other offensive equipment.
* **Vehicles**: Includes armored and transport vehicles, as well as aircraft.
* **Gear**: Tactical items such as body armor, helmets, and riot shields.
* **Other Equipment**: A diverse category comprising surveillance and communication tools, but also non-military items like office furniture.

For each category, the dataset reports both the **quantity** and **total dollar value** of items transferred, represented by variables such as `weapons_quantity`, `weapons_value`, `vehicles_quantity`, and `vehicles_value`. The total annual volume and value of aid received are summarized in `aid_quantity` and `aid_value`.

Our analysis focuses specifically on the **log-transformed total value of aid** from the **previous year**, captured in the variable `lag_log_aid_value`. This transformation helps to normalize the distribution, reducing the influence of outliers and enabling more robust statistical inference in regression models. This transformation i

---

#### 2. Crime Statistics

In addition to military aid, the dataset includes detailed **crime statistics** for each county. To allow for comparability across counties of different population sizes, crime rates are standardized per **100,000 residents**. Key variables include

* **Overall Crime Rate**: `crime_rate`
* **Crime Category Rates**: `murder_rate`, `robbery_rate`, `assault_rate`, `burglary_rate`, `larceny_rate`, and `vehicle_theft_rate`

To assess law enforcement performance, the dataset also provides **arrest rates** for each crime type, such as `arrest_rate_murder`, `arrest_rate_robbery`, and so on per **100,000 residents**.

---

#### 3. County Demographics

To contextualize both military aid and crime patterns, the dataset incorporates a range of **socioeconomic and demographic variables**, including

* **Poverty and Unemployment**: `poverty_rate`, `unempl_rate`
* **Income and Population (log-transformed)**: `log_median_income`, `log_population`
* **Gender and Racial Composition**: `share_male`, `share_black`
* **Age Cohorts**: `share_15_19_years`, `share_20_24_years`, `share_25_29_years`, `share_30_34_years`

These variables serve as potential confounders in our analysis, allowing us to statistically control for local characteristics that may influence both aid allocation and crime dynamics.

The dataset also includes identifiers that facilitate advanced modeling:

* **County and State**: `county`, `state`
* **State-Year Fixed Effects**: `state_year_interaction`, useful for panel regressions controlling for time-varying state-level factors.

---

### Preparing for Analysis: Next Steps

This thorough understanding of the dataset is essential not only for correct variable selection and interpretation but also for identifying potential sources of bias and designing credible empirical strategies. By grounding our analysis in a clear data structure, we can draw more reliable conclusions about the impacts of police militarization on crime and community outcomes.


<br/>

## Exercise 2.2 -- Exploring the Dataset in Depth: Understanding Key Metrics and Distributions

After obtaining an initial overview of the dataset and its variables, it is now time to dive deeper into the data. A detailed understanding of the dataset is essential for performing robust analyses and drawing meaningful conclusions. In this exercise, we will reload the dataset, determine the number of observations, and compute summary statistics for key variables to get a sense of the distributions and variations within the data.

Before we can start with the exercise, we have to reload the dataset using the `readRDS()` function. Simply press `check` to execute the following code chunk.

```{r "7_1"}
mil <- readRDS("data/militarization.RDS")

```

### Determining the Number of Observations

In addition to knowing the variables in the dataset, it is important to determine the number of observations (rows), as this offers information about the sample size we are working with. Having a sufficiently large dataset ensures the robustness of statistical analyses and reduces the risk of biased results due to small sample sizes.

**Task:** Print the number of rows (observations) in the `mil` dataset using the `nrow()` function.

```{r "7_2"}
# Enter your code here.
```

Upon executing the code, we see that the dataset contains **17,822 observations**. This provides a solid foundation for conducting rigorous empirical analyses.

### Computing Summary Statistics

To gain a clearer understanding of the dataset, we will compute key summary statistics for a selection of variables, including measures of **military aid distribution, crime rates, arrest rates, and socio-economic factors**. These summary statistics include:

- **Mean (Average)**: The central value of the variable.
- **Standard Deviation (SD)**: The extent of variation or dispersion in the variable.
- **Minimum (Min)**: The smallest recorded value.
- **Maximum (Max)**: The largest recorded value.
- **Observations (n)**: The total number of non-missing values for each variable.

These statistics will help us understand the range and distribution of key metrics, allowing us to identify potential outliers, trends, or data quality issues before proceeding with further analysis. To compute the metrics and show a lovely table output, we need the `dplyr`, `psych`, and `kableExtra` packages. R does not include these packages by default, so we need to load them using the `library()` command before we can use them.

**Task:** Complete the missing components in the code below to generate a summary table displaying key statistics for the selected variables.

```{r "7_3"}
library(dplyr)
library(kableExtra)
library(psych)

___ |>  # Fill in the dataset name
  select("aid_value", "aid_quantity", "weapons_value", "weapons_quantity", "vehicles_value", "vehicles_quantity", "gears_value", "gears_quantity", "other_equipment_value", "other_equipment_quantity", "crime_rate", "murder_rate", "robbery_rate", "assault_rate", "burglary_rate", "larceny_rate", "vehicle_theft_rate", "arrest_rate_murder", "arrest_rate_robbery", "arrest_rate_assault", "arrest_rate_burglary", "arrest_rate_larceny", "arrest_rate_vehicle_theft", "milex_iv", "poverty_rate", "log_median_income", "unempl_rate", "population", "share_male", "share_black", "share_15_19_years", "share_20_24_years", "share_25_29_years", "share_30_34_years") |>  
  describe() |>  
  select(mean, sd, ___, ___, n) |>  # Fill in missing summary statistics
  round(digits = 1) |>  
  kable(format = "html", col.names = c("Variable", "Mean", "SD", "Min", "Max", "Observations"), escape = FALSE) |>  
  kable_styling()
```

The table provides summary statistics for various variables related to military aid distribution, crime rates, and socioeconomic factors. Among the most important variables, the `aid_value` stands out, as it represents the total monetary value of military aid received by a county in a year. This variable shows a high degree of variability, with an average of 57,688.1 and a maximum value exceeding 152 million. The large standard deviation suggests that some counties receive significantly more aid than others, potentially reflecting different policy priorities or crime levels.

Another crucial variable is the `crime_rate`, which averages 2,473.9 crimes per 100,000 inhabitants, with a substantial range between 0 and 40,594.9. The large spread indicates that crime levels vary widely between counties, likely influenced by factors such as economic conditions, law enforcement policies, and population density. The variability in crime rates could be relevant when analyzing the potential impact of military aid on crime prevention.


Quiz: What is the most common type of crime??

(1) Murder.
(2) Burglary.
(3) Larceny.
(4) Robbery.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("common_crime_type")
```

<br/>

The `murder_rate` is also particularly significant, with a mean of 3.4 per 100,000 inhabitants and a maximum of 182.8. Although the average suggests that murder is a relatively rare occurrence, the high maximum value indicates that some counties experience significantly higher homicide rates. This discrepancy raises questions about the factors contributing to extreme cases and whether military aid allocation correlates with higher violent crime levels.

Another important factor is the `poverty_rate`, which has a mean of 16.3% but varies between 2.4% and 62%. The wide range highlights significant economic disparities between counties. Since poverty is often associated with higher crime rates, this variable is essential for understanding the broader socioeconomic context of military aid distribution. The poverty rate is calculated using a set of income thresholds that vary based on family size and composition. A family is considered to be living in poverty if its total income falls below the applicable threshold. In such cases, every individual within that household is also classified as living in poverty. It is important to note that these official poverty thresholds do not account for geographic differences in the cost of living—they are applied uniformly across all regions.

Finally, the unemployment rate (`unempl_rate`) averages 7.7%, with a maximum of 29.1%. High unemployment can contribute to economic distress and, in some cases, increased crime. The inclusion of this variable in analyses helps determine whether economic factors, rather than military aid, play a larger role in influencing crime trends.

In summary, these five variables—`aid_value`, `crime_rate`, `murder_rate`, `poverty_rate`, and `unempl_rate`—offer crucial insights into the composition of the dataset. Their wide-ranging values suggest substantial differences in economic and social conditions across counties, making them essential for further statistical analysis and interpretation.

Now that we have examined the structure of the dataset, determined the number of observations, and computed key descriptive statistics, we are well-prepared to move forward with more profound analyses. In the next exercises, we will explore relationships between military aid, crime rates, and socioeconomic conditions using econometric techniques.

This step may seem routine, but it lays the foundation for all empirical analysis. Knowing your data—not just its size and contents, but its shape and patterns—enables you to choose appropriate models, avoid misinterpretation, and build credible arguments. Robust results begin with a careful understanding of your variables.

<br/>

## Exercise 2.3 -- Graphical Analysis: Understanding Trends and Patterns

Now that we have explored summary statistics for key variables, we turn to graphical analysis to deepen our understanding of trends and patterns in the data. Visualization is a powerful tool: it helps detect anomalies, form hypotheses, and prepare for more complex statistical modeling. Before starting, reload the data by pressing `check` to run the chunk below.

```{r "8_1"}
# Load data
mil <- readRDS("data/militarization.RDS")
```

### Volume of Equipment Transferred Over Time

We begin by examining how the total volume and types of aid changed over time. This knowledge helps us understand whether the 1033 Program has expanded or shifted its focus in certain years—insights that are important when considering external shocks or policy changes. To do so we first have to create a new dataset that sums up the different types of crime for every year.

**Task:** Fill in the gaps to create a dataframe `mil_years` that contains the amount of military aid transferred in every year.

```{r "8_2"}
mil_years <- mil |>
  group_by(___) |>
  summarise(`total_volume` = sum(aid_value, na.rm = TRUE),
            `weapons_volume` = sum(___, na.rm = TRUE),
            `vehicles_volume` = sum(vehicles_value, na.rm = TRUE),
            `gears_volume` = sum(gears_value, na.rm = TRUE),
            `other_volume` = sum(other_equipment_value, na.rm = TRUE)) |>
  pivot_longer(!year, names_to = "aid_type", values_to = "aid_value") |>
  mutate(aid_type = factor(aid_type, levels =c("total_volume", "vehicles_volume", "gears_volume", "other_volume", "weapons_volume")))

head(mil_years)
```


The newly created `mil_years` dataset contains three key variables: the **year of observation** (`year`), the **type of military aid** (`aid_type`), and the **value of aid transferred** (`aid_value`). This version of the dataset adopts a *long format*, in contrast to the original *wide format*. In this structure, each row represents a single aid category in a specific year, rather than spreading categories across multiple columns. This transformation is particularly helpful for visualization purposes, as it allows us to plot trends over time more effectively. We will now use the `ggplot2` package to graphically explore how the volume of different aid types has evolved.

**Task:** Fill in the gaps in the code below to create a line plot showing the volume (cost) of the transferred equipment for each category over time.

```{r "8_3"}
library(ggplot2)

ggplot(data = mil_years, mapping = aes(x = ___, y = ___, colour = aid_type)) +
  geom_line(linewidth = 1.2) +
  scale_y_continuous(breaks = c(0,1e8,2e8,3e8,4e8,5e8),
                     labels = c("0","100M","200M","300M","400M","500M")) +
  labs(y = "Aid value (USD)", x = "", title = "Volume of received military aid over time") +
  theme_bw() +
  theme(legend.position = "inside",
        legend.position.inside = c(0.15,0.8),
        legend.title = element_blank())

```

The graph illustrates a clear upward trend in the total value of military equipment transferred through the 1033 Program from 2007 to 2012. We observe a particularly pronounced increase in transferred equipment after 2010.

A key driver of this growth is the Vehicles category, which shows a substantial increase starting in 2010 and continues to grow at a rapid pace. This evidence suggests that a large portion of the total equipment value consists of vehicle-related transfers, such as armored vehicles, trucks, or other transport-related assets. The Other Equipment category also experiences a noticeable increase after 2010. While not as steep as the rise in vehicle transfers, it indicates a growing distribution of non-weapon military resources, which could include communication tools, surveillance equipment, or protective gear. The significant increase in military equipment deliveries to U.S. police agencies after 2010 can largely be attributed to the withdrawal of U.S. forces from conflict zones such as Iraq and Afghanistan. These troop withdrawals rendered large quantities of military equipment surplus (al-Hamid, 2012). Instead of incurring high costs for demilitarization or storage, the U.S. Department of Defense decided to redistribute these excess supplies to local police agencies through the 1033 Program. The Gears category follows a more gradual and steady increase over the years. The pattern suggests that the transfer of specialized tactical equipment and clothing has been consistently growing, albeit without the dramatic spikes seen in vehicles or other equipment. Interestingly, the Weapons category remains relatively low and stable throughout the observed period. The relatively low and stable value of transferred weapons compared to the sharp rise in vehicles and other equipment can, at least in part, be explained by the significant cost differences between these categories. Firearms, while an essential component of military and law enforcement operations, are considerably cheaper than armored vehicles, high-tech surveillance systems, and other advanced military-grade equipment. The procurement expenses of vehicles, particularly armored trucks and personnel carriers, are substantially higher than those of standard firearms, leading to a much larger impact on the total value of transferred military aid. Similarly, high-tech electronics such as communication tools and surveillance equipment often come with high price tags, further contributing to the increasing overall value of distributed resources. This suggests that the focus of the 1033 Program has been on equipping police forces with expensive operational and logistical assets rather than simply providing them with additional weapons.

### Classification of High- and Low Recipient Counties

We now distinguish between counties that receive military equipment more or less often. Counties are classified based on the number of years they received military aid from the 1033 program:
- **High-recipient counties:** `recieving_years >= 3`
- **Low-recipient counties:** `recieving_years <= 2`

Now, compute the mean crime rate and aid volume for each group and year.

**Task:** Create a data frame `mil_recieving` that contains the means of `crime_rate` and `lag_log_aid_value` for high and low recipient counties for each year. Remove `NA` values in the mean computation.

```{r "8_4"}
mil_recieving <- mil |>
  mutate(hi_low = ifelse(recieving_years >= ___, "High recipients", "Low recipients")) |>
  group_by(hi_low, year) |>
  summarise(crime_rate = mean(crime_rate, na.rm = TRUE),
            aid_value = mean(lag_log_aid_value, na.rm = TRUE))

head(mil_recieving)
```


### Crime Rate Trends

Now, we visualize the average crime rate over time for high- and low-recipient counties. Just press `check` to run the chunk below.

```{r "8_5"}
ggplot(data = mil_recieving, mapping = aes(x = year, y = crime_rate, linetype = hi_low)) +
  geom_line(size = 1.2) +
  scale_y_continuous(breaks = c(0,500,1000,1500,2000,2500,3000,3500),
                     limits = c(0,3500)) +
  labs(y = "Crime", x = "", title = "Comparison of crime rate (per 100,000) between \n low- and high-recipient counties") +
  theme_bw() +
  theme(legend.position = "inside",
        legend.position.inside = c(0.2,0.3),
        legend.title = element_blank())
```

The graph illustrates the difference in crime rates per 100,000 inhabitants between counties that received high and low levels of military equipment through the 1033 Program from 2007 to 2012. The y-axis represents the crime rate, while the x-axis indicates the years. The two lines depict the trends for counties classified as high recipients and low recipients of military aid.

Over the observed period, both groups of counties experienced a general decline in crime rates. The evidence suggests that crime reduction occurred independently of the level of military equipment received. However, counties classified as high recipients consistently had higher crime rates compared to low-recipient counties. This difference remained stable over time, indicating that counties receiving more military aid were likely already experiencing higher crime levels before the transfers took place.

Despite the overall downward trend, the gap between high- and low-recipient counties did not significantly close. This parallel movement suggests that while crime was decreasing across both groups, the additional military aid did not appear to accelerate the decline in crime for high-recipient counties compared to low-recipient ones. Interestingly, after 2010, the crime rate in low-recipient counties appears to stabilize slightly, whereas high-recipient counties continue their gradual decrease. This phenomenon could indicate differing law enforcement strategies or other socio-economic factors affecting crime trends in these regions.

The persistently higher crime rates in high-recipient counties might suggest that these areas were allocated more military aid due to their pre-existing crime problems, rather than the aid itself influencing crime levels. Furthermore, the relatively stable gap between the two groups implies that the presence of military equipment alone may not be a decisive factor in crime reduction. Instead, broader societal changes, improved policing tactics, or economic factors may play a more significant role in shaping these trends. While the graph provides a clear comparison between the two groups, it does not offer conclusive evidence that increased military aid directly impacts crime reduction. Further statistical analysis would be necessary to establish any causal relationship between the distribution of military equipment and changes in crime rates.

### Military Aid Distribution Trends

Now, create a second plot that visualizes the amount of aid received by high- and low-recipient counties over time.

```{r "8_6"}
ggplot(data = mil_recieving, aes(x = year, y = aid_value, linetype = hi_low)) +
  geom_line(size = 1.2) +
  labs(y = "Log Aid Volume", x = "", title = "Comparison of received military aid between \n low- and high-recipient counties") +
  theme_bw() +
  theme(legend.position = "inside",
        legend.position.inside = c(0.3,0.85),
        legend.title = element_blank())
```

The graph displays the difference in received military aid between counties classified as high and low recipients from 2007 to 2012. The y-axis represents the aid volume, while the x-axis indicates the years. Two distinct lines illustrate the trends for counties that received high levels of military equipment and those that received low amounts under the 1033 Program.

Throughout the observed period, high-recipient counties consistently received significantly more military aid than low-recipient counties. The aid volume for high-recipient counties initially declines from 2007 to 2008 but then stabilizes and begins to rise again from 2010 onward, showing a sharp increase in 2012. This pattern suggests fluctuations in the allocation process, possibly influenced by shifts in military surplus availability or changes in policy regarding equipment distribution.

Low-recipient counties, in contrast, received substantially less aid throughout the entire period.  Although there are minor variations in the volume of aid distributed to these counties, the overall trend remains relatively stable, with only a slight increase toward the end of the period.

The divergence between high and low recipients is particularly noticeable, reinforcing the idea that military aid is not evenly distributed but rather concentrated in specific regions. The significant increase in aid to high-recipient counties after 2010 aligns with broader trends in military surplus distribution, possibly linked to the withdrawal of U.S. forces from conflict zones, which made more equipment available for redistribution (al-Hamid, 2012).

Overall, the graph highlights a growing disparity in military aid distribution, with high-recipient counties receiving an increasing share over time. While low-recipient counties experience only marginal increases, high-recipient counties see a more pronounced rise, especially from 2010 onward. This trend suggests that the allocation of military equipment under the 1033 Program is likely driven by specific demand factors, law enforcement needs, or other strategic considerations.

These visual explorations are more than just descriptive—they offer first insights into potential relationships, trends, and anomalies. They can spark hypotheses (e.g., aid may be reactive to crime surges) and help identify necessary controls for later econometric models. A careful visual diagnosis now lays the groundwork for rigorous causal inference later.

<br/>

## Exercise 3 -- Identification and Causal Inference

In this exercise, we aim to systematically investigate the causal effect of military aid on crime rates. The central research question is, does the transfer of military-grade equipment to local law enforcement agencies reduce crime?

Rather than assuming a direct or immediate impact, we approach this question incrementally, starting with simple descriptive regressions and gradually introducing more rigorous identification strategies. This procedure allows us to carefully examine potential threats to causal inference—most notably selection bias and unobserved confounders, which may simultaneously influence both the allocation of military aid and crime levels.

We improve our analysis step by step: starting with simple two-variable regressions, then adding control variables, fixed effects, and finally using an instrumental variable (IV) approach. This progression not only sharpens our understanding of the relationship’s direction and magnitude but also helps us evaluate how sensitive our results are to different modeling assumptions.

By following this structured approach, we aim to move beyond surface-level correlations to a more credible, evidence-based assessment of whether and how military aid influences public safety outcomes.

### Structure

3.1 — Military Aid and Crime: A Fixed Effects Approach

3.2 — Standard Errors and Clustering: Ensuring Robust Inference

3.3 — Instrumental Variables: Introducing the Method through Simulation

3.4 — Instrumental Variables and Military Aid: Causal Estimates in Panel Data

<br/>

## Exercise 3.1 --  Military Aid and Crime: A Fixed Effects Approach

In this exercise, we turn to estimating the relationship between military aid and crime rates using econometric methods. Our goal is to assess whether military transfers under the 1033 Program have a measurable effect on local crime outcomes.  However, unobserved factors likely confound this relationship—counties receiving more aid may differ systematically in ways that also affect crime.

To address this endogeneity problem, we incorporate control variables as well as fixed effects that account for time-invariant and time-specific influences. In particular, we use state-year fixed effects to control for broader policy trends or shocks that vary across time and location. This strategy helps isolate variation in military aid that is plausibly exogenous to local crime developments.

This step is essential not only for improving the credibility of our estimates but also for moving from correlation to a more plausible causal interpretation. As you work through the exercise, reflect on the assumptions behind fixed effects models—and consider what kinds of bias they can and cannot address.

But before we can start with the first regression of our analysis we have to load our data again. Just press `check` to run the chunk below.

```{r "10_1"}
# Load data
mil <- readRDS("data/militarization.RDS")
```


Since our primary interest lies in estimating the effect of military aid on crime rates, we begin with a baseline regression model that captures this core relationship. At this stage, we focus on the bivariate association between the two variables, without yet accounting for potential confounders.

To estimate this model, we use the `feols()` function from the `fixest` package. As our explanatory variable, we use the log-transformed value of military aid received in the previous year (`lag_log_aid_value`). This log transformation helps address the skewed distribution of aid across counties and ensures that extreme values do not disproportionately influence the results. We also lag the aid variable by one year to reflect the realistic assumption that it may take time for newly acquired equipment to influence crime outcomes. Immediate effects are unlikely, as implementation, training, and operational deployment introduce a natural delay.

This initial regression provides a simple but informative starting point. It allows us to assess whether a basic association exists, setting the stage for more robust causal analysis in subsequent exercises.

**Task:** Run a simple regression of `crime_rate` on `lag_log_aid_value` and save it under `mod_simple`. Use the `feols()` function to calculate the regression. Then show the results of the regression using the given code with the `modelsummary` function. Don't forget to load the `fixest` and `modelsummary` packages with the `library()` function first.

```{r "10_2"}
# Enter your code here.

modelsummary(list("Simple Regression" = mod_simple), 
             output = "kableExtra", 
             gof_map = c("nobs"), 
             stars = TRUE) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "crime_rate" = 1), monospace = TRUE)
```



Quiz: How can we interpret the coefficient of the lagged, logarithmized  military aid in the regression if we assume a causal relationship?

(1) 10$ more aid leads to approximately 7.5 more crimes.
(2) 10% more aid leads to approximately 7.5 less crimes.
(3) 10$ more aid leads to approximately 0.75% more crimes.
(4) 10% more aid leads to approximately 7.5 more crimes per 100,000 population.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("coeffiecient_interpretation_3")
```

<br/>

The initial regression yields a counterintuitive result: contrary to the assumption that better-equipped police forces should reduce crime, the data suggest a positive association between military aid and crime rates. In other words, counties receiving higher levels of aid tend to report higher—not lower—crime levels. However, this finding should not be interpreted as causal.

This result likely reflects an endogeneity problem, most notably omitted variable bias. Key determinants that influence both the receipt of military aid and crime levels—such as socioeconomic conditions or pre-existing trends in public safety—may be excluded from the model. As a result, the estimated coefficient on military aid could be capturing the influence of these unobserved factors rather than the true effect of aid itself.

A common scenario illustrating this bias involves county population size. Urban areas tend to have larger police departments that are more likely to request and receive substantial aid through the 1033 Program. These same areas also tend to exhibit higher crime rates, independent of aid. If we fail to control for population, the estimated relationship between aid and crime will be upward biased, conflating the effect of aid with the underlying influence of urbanization. This confounding relationship is shown in the DAG (Directed Acyclic Graph) below, where population influences both military aid and crime, creating a misleading connection if we don't consider it.

![DAG of our assumed data model](data/images/DAG_population.jpg){width=700px style="display: block; margin-left: auto; margin-right: auto;"}
<p style="text-align: center; font-size: 0.8em; color: gray;">
Source: Own illustration
</p>

To address the omitted variable bias, the next step is to augment the regression model with a set of control variables that more accurately reflect the socioeconomic and demographic environment of each county. By incorporating these covariates, we aim to account for observable differences that may simultaneously influence both the likelihood of receiving military aid and baseline crime rates. Specifically, we include the logarithm of population size to control for scale effects, the poverty rate to capture economic deprivation, the logarithm of median household income, and the unemployment rate as indicators of broader economic conditions. Additionally, we control for demographic composition, including the proportion of residents who are male, identify as black, and the relative sizes of key age cohorts (15–19, 20–24, 25–29, and 30–34 years).

By integrating these variables into the regression model, we reduce the risk of omitted variable bias stemming from observable characteristics. This adjustment enhances the credibility of our empirical strategy and moves us one step closer to estimating the true causal effect of military aid on crime, rather than a distorted relationship driven by confounding influences.

**Task:** Expand the first regression analysis with the control variable `log_population` and save it under `mod_pop`.

```{r "10_3"}

# Enter your code here


mod_control <- feols(crime_rate ~ lag_log_aid_value + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years, data = mil)

modelsummary(list("Simple Regression" = mod_simple, 
                  "Population Control" = mod_pop, 
                  "Control All" = mod_control), 
             output = "kableExtra", 
             gof_map = c("nobs"), 
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 3), monospace = TRUE)
```


In the population control model, the inclusion of `log_population` substantially reduces the coefficient of military aid from 75.704 in the simple regression to 19.661. This attenuation suggests that population size, rather than aid itself, drove part of the initially observed relationship between military aid and crime. Larger, more urban counties tend to receive more aid—and also tend to experience higher crime rates—thus inflating the initial estimate due to a confounding effect. This pattern is consistent with previous findings that crime rates tend to be lower in smaller, rural counties (Osgood & Chambers, 2000). Nevertheless, even after controlling for population, the coefficient for military aid remains positive and statistically significant—a result that continues to run counter to the intuitive expectation that better-equipped police forces should deter crime.

In the Control All model, we further extend the specification by adding a broader set of control variables to better account for socioeconomic and demographic heterogeneity across counties. These include the poverty rate, log median income, unemployment rate, and indicators for gender composition, racial identity, and age group distribution. This comprehensive specification aims to address additional observable confounders that could simultaneously influence both military aid allocation and crime dynamics.

Despite this richer set of controls, the coefficient for `lag_log_aid_value` increases slightly to 20.230, and remains statistically significant.This persistence indicates that even after considering many visible factors, there may still be some bias left—probably because of unmeasured influences like the quality of institutions, past crime patterns, or local political situations that aren't directly included in the data. As such, while the inclusion of control variables improves the credibility of our estimates, it does not yet resolve all endogeneity concerns. Further steps—such as incorporating fixed effects or instrumental variables—will be needed to more convincingly isolate the causal impact of military aid.

Because counties differ in many unobservable ways, it is important to control for characteristics that are constant over time but vary across regions. To achieve this, we introduce fixed effects into our regression model. The goal is to focus solely on within-county variation over time, rather than comparing differences between counties, which may be driven by persistent, unmeasured factors. Since counties are categorical, not numeric, fixed effects are necessary to properly control for them in the regression.


### Why Use Fixed Effects?

Fixed effects models are particularly well suited for panel data because they help control for unobserved, time-invariant characteristics that differ across counties but remain constant within them over time. This approach allows us to estimate the effect of military aid based on within-county changes, rather than between-county differences that might be confounded by stable local factors.

For instance, geographic location is a structural feature that can influence both crime patterns and the allocation of military aid. Counties near international borders may experience different crime dynamics than those in the interior. Simple OLS models would not adequately account for such variation, potentially leading to biased estimates (Glaeser & Sacerdote, 1999). By adding county fixed effects, we take into account these constant, unseen factors—like location, past crime rates, or local systems—making our conclusions more reliable.

### Implementing Fixed Effects in R

To include county-specific fixed effects, we cannot simply add the `county` variable as a numerical regressor. Since county identifiers (like FIPS codes) are categorical, treating them as continuous variables would produce meaningless results. Instead, we must introduce **dummy variables** for each county—an approach that effectively controls for all time-invariant county-level traits.

Fortunately, the `feols()` function in the `fixest` package simplifies this process. Rather than manually creating dummies, we include the fixed effects directly in the regression formula using a pipe (`|`). For example:

```r
feols(outcome ~ treatment + controls | fixed_effect, data = my_data)
```

This syntax automatically includes one dummy variable for each county while suppressing their individual coefficients in the output. This method keeps the regression table clear and focused on the variables of interest—without sacrificing analytical rigor.

**Task:** Expand the `mod_control` regression from the previous exercise with county level fixed effects and store the result in `mod_county`.


```{r "10_4"}
# Enter your code here.
modelsummary(list("Control" = mod_control, 
                  "County FE" = mod_county), 
             output = "kableExtra", 
             gof_map = c("nobs"), 
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 2), monospace = TRUE)
```

### Expanding to State-Year Fixed Effects

To further improve the credibility of our estimates, we now introduce state-year interaction fixed effects. While county fixed effects control for time-invariant local characteristics, counties within the same state may still be exposed to shared, time-varying shocks—such as changes in state laws, public safety funding, or judicial practices. Including a fixed effect for each state-year combination helps capture these broader policy dynamics and removes variation that could otherwise bias our results (Arellano, 2003).

*Task:* Expand the `mod_county` model with `state_year_interaction` fixed effects. Save the results under `mod_fe`.

```{r "10_5"}
# Enter your code here.
modelsummary(list("Control" = mod_control, 
                  "County FE" = mod_county, 
                  "All Fixed Effects" = mod_fe), 
             output = "kableExtra",
             gof_map = c("nobs"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)  |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 3), monospace = TRUE) 
```


The addition of state-year fixed effects in the All Fixed Effects model further refines our analysis by accounting for unobserved state-level variation over time. Compared to earlier models, the number for lag_log_aid_value goes down slightly—suggesting that some of the earlier connection was probably influenced by trends that are specific to each state over time.

However, the coefficient remains statistically insignificant and positive, which continues to challenge the theoretical expectation that military aid should deter crime. This ongoing issue indicates that, even after considering a wide range of known factors that change over time, there might still be leftover bias in the model. Possible explanations include selection effects in aid allocation, unmeasured institutional characteristics, or measurement error in the aid data itself. These factors likely distort the estimated relationship and highlight the limitations of observational approaches in fully capturing causal dynamics.


### Conclusion

By gradually moving from a basic OLS regression to a detailed fixed effects model that includes both county and state-year interacted fixed effects, we greatly enhance the trustworthiness of our estimates.This method accounts for unchanging county traits and differences in state policies over the years, which helps minimize errors from missing variables and strengthens our identification strategy (Arellano, 2003). As the model becomes more refined, the coefficient for military aid drops sharply and loses statistical significance in both fixed effects specifications. This shift suggests that earlier positive results were likely driven by unobserved confounders. However, the coefficient remains positive—indicating that selection bias or other latent factors may still influence the estimated relationship.

A final methodological concern involves the reliability of our standard errors. Since counties are nested within states and policy shocks or crime trends may be correlated across counties in the same state, standard errors clustered at the state level provide a more appropriate correction. This approach accounts for the arbitrary correlation of errors among states over time, reflecting the clustering strategy used in the original research paper. In the next exercise, we will re-estimate our fixed effects model using state-level clustered standard errors to ensure that our conclusions about statistical significance remain robust to this form of error dependence.

<br/>

## Exercise 3.2 -- Standard Errors and Clustering: Ensuring Robust Inference

In this exercise, we take a closer look at the role of standard errors in regression analysis, with a particular focus on the importance of using clustered robust standard errors. While regression coefficients often receive the most attention, the validity of statistical inference—such as hypothesis testing and confidence intervals—hinges critically on how we estimate those standard errors.

In the context of our panel dataset, repeated observations within geographic units (such as counties nested within states) introduce the possibility of within-cluster correlation over time. If this correlation is ignored, standard errors are likely to be underestimated, leading to overconfident conclusions and inflated significance levels.

Building on the fixed effects models developed in Exercise 3.1, this step ensures that our conclusions regarding statistical significance are not an artifact of incorrect error assumptions. By the end of this exercise, you will understand how to specify clustered standard errors properly in R using the fixest package and why this adjustment is a vital part of credible empirical analysis—especially in policy-relevant applications such as evaluating the impact of police militarization.

Before we continue we have to reload our data. Just press `check` to run the chunk below.

```{r "11_1"}
# Load data
mil <- readRDS("data/militarization.RDS")
```

A key assumption of Ordinary Least Squares (OLS) estimation is that error terms are independently and identically distributed (i.i.d.). This assumption is important because it affects how we calculate standard errors, which are used to check the trustworthiness of p-values, confidence intervals, and hypothesis tests (Stock & Watson, 2020).

However, our panel data setting likely violates this assumption. Counties within the same state often experience similar policy environments, economic cycles, or institutional conditions—factors that can introduce correlated error terms across counties within a state. When such within-cluster dependence is ignored, conventional standard errors tend to underestimate true variability, potentially leading to spuriously significant results (Moulton, 1986).

To address this, we use cluster-robust standard errors, which relax the i.i.d. assumption and allow for the arbitrary correlation of residuals within clusters.This method, called the Cluster-Robust Variance Estimator (CRVE), assumes that clusters are independent from each other but allows for connections among the data points within each cluster (Arellano, 1987). In our case, we follow the original study’s approach and cluster at the state level—a natural choice given that state-level policies and trends can affect all counties within a state simultaneously.

A known limitation of CRVE is that it performs best with a sufficiently large number of clusters. Fortunately, our dataset includes 49 states, which is generally considered adequate to ensure the consistency and reliability of CRVE-based standard errors.

In practice, computing clustered standard errors in R using the fixest package is straightforward: simply add the argument cluster = ~ state in the feols() function call. This approach allows us to implement the correction efficiently and ensure that our inference remains valid in the presence of intrastate correlation.

**Task:** Fill in the gap to compute the regression model `mod_cluster` with cluster robust standard errors. The fixed effects model from the previous exercise is also computed to compare the results.

```{r "11_2"}
mod_fe <-  feols(crime_rate ~ lag_log_aid_value + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = mil)

mod_cluster <-  feols(crime_rate ~ lag_log_aid_value + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = mil, ___)


modelsummary(list(mod_fe, mod_cluster), 
             output = "kableExtra",
             gof_map = c("nobs"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)
```


Despite common expectations, applying the Cluster-Robust Variance Estimator (CRVE) does not always result in larger standard errors. In our case, as the table above shows, the standard error for lag_log_aid_value actually decreases slightly—from 1.313 (conventional) to 1.185 (CRVE). Typically, we introduce cluster-robust methods to correct downward-biased standard errors, but there are also cases when the standard errors are upward biased. It can arise when within-cluster correlation is weak, negative, or when standard errors from conventional methods are overly conservative.

One explanation is that the intra-state correlation in our residuals is relatively low. In such cases, the variability of errors across counties within a state may offset each other, reducing the total variance. On the other hand, using multiple fixed effects in our model (like county and state-year interactions) might make the usual standard errors seem larger than they really are, especially if we've already accounted for cross-sectional dependence. In such settings, cluster-robust estimators—designed to better reflect complex panel structures—can yield more accurate and sometimes smaller standard errors (Cameron & Miller, 2015).

However, CRVE also has limitations—most notably its sensitivity to unbalanced cluster sizes. In our dataset, the number of observations per state varies widely: Washington D.C. has only 6, while Texas has over 1,500. Such imbalances can distort standard error estimates, as large clusters may dominate the inference, while small ones are underrepresented (MacKinnon & Webb, 2017).

To address this, we turn to the Wild Cluster Bootstrap—a method that remains valid in the presence of heteroskedasticity and highly unequal cluster sizes. It creates pseudo-samples by rescaling the residuals within each cluster using random weights (typically Rademacher-distributed: ±1). This resampling preserves key statistical properties while avoiding overreliance on asymptotic approximations.

There are two main variants:

* **WCU (Unrestricted)**: uses residuals from the full model,

* **WCR (Restricted)**: imposes the null hypothesis, such as β = 0, during the estimation of residuals. The WCR variant typically delivers more accurate inference in finite samples because every bootstrap draw embeds the null directly.

The procedure proceeds by calculating a bootstrap t-statistic for each draw, building an empirical distribution of the test statistic. The resulting p-value is based on the share of bootstrapping t-values that are more extreme than the observed ones.  Importantly, this method avoids relying solely on standard errors and allows robust inference even when the number of clusters is small.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Why the Wild Cluster Bootstrap can be fast")
```

In our case, however, implementing the wild-cluster bootstrap directly on our fixed effects model using `boottest()` from the `fwildclusterboot` package leads to a technical error. As a workaround, we use the `demean()` function from the `fixest` package to remove fixed effects manually before applying bootstrapping. This function subtracts the group means defined by fixed effects from the relevant variables, effectively replicating the behind-the-scenes operations of OLS with fixed effects. It allows us to retain the structure of a within-transformation while gaining full control over the regression process.


```{r "11_3"}

library(fwildclusterboot)

dqrng::dqset.seed(12345678)

set.seed(1234)


df_demeaned <- demean(mil[,c("crime_rate","lag_log_aid_value","log_median_income", "poverty_rate", "unempl_rate", "log_population", "share_male", "share_black", "share_15_19_years", "share_20_24_years", "share_25_29_years", "share_30_34_years")], mil[,c("county", "state_year_interaction")])

df_demeaned <- df_demeaned |>
  mutate(state = mil$state)

mod_demeaned <- feols(crime_rate ~ lag_log_aid_value + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years, data = df_demeaned)


mod_boot <- boottest(mod_demeaned, clustid = c("state"), B = 9999, param = "lag_log_aid_value")



modelsummary(list("Conventional" = mod_fe, "CRVE" = mod_cluster, "Fast Wild Boot" = mod_boot), 
             output = "kableExtra",
             coef_map = c("lag_log_aid_value" = "lag_log_aid_value",
                          "1*lag_log_aid_value = 0" = "lag_log_aid_value"),
             gof_map = c("nobs"),
             stars = TRUE,
             tidy = tidy_custom,
             statistic = "[{conf.int.2.5..}, {conf.int.97.5..}] <br> Width: {conf.width}"
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 3))

```


Because the Fast Wild Bootstrap procedure is designed specifically for hypothesis testing, it does not provide a conventional standard error. Instead, it returns a bootstrap-based confidence interval, which is derived from the distribution of test statistics under the null hypothesis. The 95% confidence interval is computed by inverting a series of hypothesis tests: it consists of all values $\beta_0$ for which the null hypothesis $H_0 : \beta = \beta_0$ is not rejected at the 5% level. In practical terms, this means the interval includes all parameter values for which the corresponding p-value is greater than or equal to 0.05. This inversion approach is particularly well-suited to small samples or settings with few clusters, where normal approximations can be unreliable.

To check how reliable our estimates are, we look at the 95% confidence intervals and their sizes across estimation methods for the coefficient on `lag_log_aid_value.` As shown in the table above, the bootstrap interval is slightly narrower than both the conventional and cluster-robust (CRVE) intervals, with a total width of 4.727, compared to 5.148 and 4.767, respectively.

Since the `modelsummary()` function does not offer built-in support for calculating and displaying confidence interval widths, we define a custom `tidy()` method to extract the necessary values—specifically, the coefficient estimates, confidence intervals, and calculated widths. We then pass this function to the `modelsummary()` call using the argument `tidy = tidy_custom`, ensuring that the table accurately reflects the output structure.

The fact that the CRVE and bootstrap intervals are similar shows that the differences in the number of observations from each state do not significantly affect our conclusions. Although some states (e.g., Texas) contribute over 1,500 observations while others (e.g., D.C.) contribute fewer than 10, the consistency in the results supports the use of both methods.

Given the negligible difference between the CRVE and Wild Bootstrap results, and the fact that the original study by Bove & Gavrilova also relies on CRVE, we choose to proceed with the CRVE method for the remainder of our analysis. It is computationally more efficient, well-established in applied work, and performs reliably in our setting.

The widths of the confidence intervals for all three methods are quite close, ranging from 5.148 (conventional) to 4.727 (bootstrap), which supports the strength of our results even with different ways of analyzing the data.

This exercise has demonstrated that careful consideration of standard error estimation is essential for valid inference and that clustered and bootstrap-based approaches provide helpful tools for addressing complex error structures in panel data.

<br/>

## Exercise 3.3 -- Instrumental Variables: Introducing the Method through Simulation

In many empirical settings, unobserved variables can lead to biased and inconsistent estimates in regression models. A common case arises when an explanatory variable is correlated with an unobserved confounder that also affects the outcome. Even when fixed effects are used to account for time-invariant unobserved heterogeneity, this process does not resolve bias from unmeasured time-varying factors.

To illustrate how such endogeneity can be addressed, this exercise introduces the method of instrumental variable (IV) regression using a simulated dataset. Our goal here is not to estimate the true causal effect of military aid in the real world but to understand the mechanics and intuition of IV estimation in a controlled, stylized setting (Angrist & Pischke, 2008).

We assume a simple theoretical model in which an unobserved factor—crime experience—influences both the explanatory variable (military aid) and the outcome (crime rate). Since this unobserved variable cannot be directly measured or controlled for, any direct regression of crime on military aid will be affected by omitted variable bias.

To address this, we simulate an instrumental variable, military expenditure (mil_ex), which influences the amount of aid distributed (mil_aid) but is assumed to have no direct effect on local crime rates—only an indirect one via its impact on aid. This model reflects the idea that national military spending may determine how much surplus equipment is available but not which counties experience crime.

The directed acyclic graph (DAG) below visualizes these assumed relationships. It shows that military aid is determined by both military expenditure and crime experience, while crime experience also directly affects crime rates.  We cannot simply include crime experience as a control variable in this example because it is unobservable.

![DAG of our assumed data model](data/images/DAG_IV.jpg){width=700px style="display: block; margin-left: auto; margin-right: auto;"}
<p style="text-align: center; font-size: 0.8em; color: gray;">
Source: Own illustration
</p>

To demonstrate the IV method, we now simulate a dataset based on this structure:

```{r "12_1"}
# Set seed for reproducibility
set.seed(123)

# Number of observations
n <- 10000

# Unobserved variable (crime experience)
crime_experience <- rnorm(n, mean = 2, sd = 1)

# Instrumental variable (military expenditure)
mil_ex <- rnorm(n, mean = 5, sd = 1)

# Endogenous explanatory variable (military aid), influenced by mil_ex and crime_experience
mil_aid <- 0.8 * mil_ex + 1.5 * crime_experience + rnorm(n, mean = 0, sd = 1)

# Dependent variable (crime rate), influenced by mil_aid and crime_experience
crime_rate <- 1.5 + (-0.5 * mil_aid) + 2 * crime_experience + rnorm(n, mean = 0, sd = 1)

# Combine into a data frame
dat <- data.frame(
  crime_rate = crime_rate,
  mil_aid = mil_aid,
  mil_ex = mil_ex,
  crime_experience = crime_experience  # This is unobserved in the analysis
)

head(dat)
```


This dataset consists of 10,000 simulated observations. The variable `crime_experience` is an unobserved confounder that affects both `mil_aid` and `crime_rate.` The instrumental variable `mil_ex` influences only `mil_aid`, not `crime_rate` directly. In this controlled setup, the true causal effect of military aid on crime is –0.5.

This example is intentionally kept simple: no additional control variables or fixed effects are included, so we can focus solely on understanding how IV regression helps solve the endogeneity problem. In real applications, of course, these relationships would be more complex and the assumptions harder to justify.

By using a instrument like mil_ex, which creates changes in military aid that aren't connected to hidden crime trends, we can separate the outside influence of mil_aid and more accurately measure its impact on crime—at least based on the conditions we've set in our simulation.

The next step is to estimate our first regression models.

**Task:** Estimate the effect of military aid on crime rate using the feols() function, both with and without controlling for crime experience and save it under `reg_ols_simple` and `reg_ols_control`.

```{r "12_2"}
# Enter your code here.
modelsummary(list("Simple" = reg_ols_simple, 
                  "Control" = reg_ols_control), 
             output = "kableExtra",
             gof_map = c("nobs"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 2), monospace = TRUE)
```


In the first model (Simple), we estimate a basic OLS regression without controlling for the unobserved variable `crime_experience`. The coefficient for `mil_aid` is estimated at 0.256, suggesting a small positive relationship. However, since `crime_experience` is omitted—and in our data-generating process it affects both `mil_aid` and `crime_rate`—this estimate suffers from omitted variable bias. The model mistakenly attributes part of the effect of `crime_experience` to `mil_aid`, resulting in a biased and inconsistent estimate.

In the second model (Control), we include `crime_experience` directly as a regressor. The estimated effect of `mil_aid` now shifts to –0.514, which is very close to the true causal effect of –0.5 defined in our simulation. The coefficient for `crime_experience` is also close to its true value (2.004 vs. 2.0). This finding confirms that omitted variable bias is the key issue in the first model and that the second model yields unbiased estimates—if the confounder is observed.

Of course, in real-world applications we cannot directly observe `crime_experience`. This simulation highlights the fundamental challenge: without access to key unobserved confounders, even well-specified OLS models may yield misleading results. It points out the need for alternative estimation strategies, such as IV regression, to recover causal effects in the presence of unobserved confounding.


### First-Stage Regression

Instrumental variable regression—specifically two-stage least squares (2SLS)—requires an additional variable known as an instrument. A valid instrument must satisfy two core conditions:

* **Relevance**: The instrument must be strongly correlated with the endogenous regressor (here, `mil_aid`). This requirement ensures that the instrument actually explains a nontrivial share of its variation.
* **Exogeneity**: The instrument must be uncorrelated with the error term in the structural equation. That is, it may influence the dependent variable (`crime_rate`) only indirectly—through its effect on `mil_aid`.

While the relevance condition can be tested empirically (e.g., via a weak instrument test), exogeneity cannot be directly verified using statistical tools. Instead, it relies on substantive reasoning about the data-generating process. In our simulated dataset, we know by construction that `mil_ex` affects only `mil_aid` and not `crime_rate` directly—thus satisfying the exogeneity assumption by design.

In the first stage of IV regression, we regress the endogenous variable `mil_aid` on the instrument `mil_ex`. This step extracts the exogenous variation in `mil_aid` attributable to the instrument. The fitted values from this regression are then used in the second stage, where they replace the original endogenous regressor in the structural equation.

**Task:** Conduct a regression of `mil_aid` on `mil_ex` by using the feols() function and the `dat` dataset. Save the results in the variable `reg_first_stage` and add a new variable `fit_mil_aid` to `dat` that contains the fitted values of that regression.

```{r "12_3"}
# Enter your code here.

modelsummary(list("First Stage" = reg_first_stage), 
             output = "kableExtra",
             gof_map = c("nobs"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1,"mil_aid" = 1), monospace = TRUE)
```

The first-stage regression isolates the variation in military aid that can be explained by our instrumental variable, military expenditure. The estimated coefficient of 0.806 indicates a strong and positive relationship between the two variables.

### Second-Stage Regression

With the fitted values from the first-stage regression, we now estimate the second stage of our IV model. In this step, we use the predicted values of the mil_aid variable, which only reflect the part explained by the instrument, to see how they affect the crime_rate variable.

**Task:** Conduct the second stage regression and save it under `reg_second_stage`.

```{r "12_4"}
# Enter your code here.
modelsummary(list("Simple" = reg_ols_simple, "Control" = reg_ols_control, "Second Stage" = reg_second_stage), 
             output = "kableExtra",
             gof_map = c("nobs"),
             coef_rename = c("fit_mil_aid" = "mil_aid"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 3), monospace = TRUE)
```

The IV estimate from the second-stage regression is –0.494, which is remarkably close to the true causal effect of –0.5 specified in our data-generating process. This result confirms that the IV approach successfully removes the bias caused by the unobserved confounder crime_experience.

We anticipate a slight difference from the control model because IV regression relies only on the exogenous variation in mil_aid that is attributable to the instrument (mil_ex), not on the full range of observed values. As a result, it often produces less precise but more causally valid estimates, particularly when the instrument is strong and the assumptions hold.

### 2SLS Regression in One Step

We can use a more efficient and syntactically clean approach by specifying the entire IV model in a single step with the `feols()` function from the `fixest` package, instead of estimating the two stages of a 2SLS regression separately. This method streamlines the estimation process and ensures that uncertainty from both stages is correctly taken into account.

To set up a 2SLS model in `feols()`, we use the formula syntax:

```r
dependent_variable ~ covariates | fixed_effects | endogenous ~ instrument
```

In our case, since we are not including any additional covariates or fixed effects, the formula simplifies to:

```r
crime_rate ~ 1 | mil_aid ~ mil_ex
```

In this case, `1` is used instead of the endogenous regressor in the main formula to prevent repetition, and the last part `(mil_aid ~ mil_ex)` indicates to `feols()` which variable is endogenous and which one to use as an instrument.

**Task:** Estimate the 2SLS model in one step using the `feols()` function, and assign the result to an object called `reg_iv`.

```{r "12_5"}
# Enter your code here.
modelsummary(list("Simple" = reg_ols_simple, 
                  "Control" = reg_ols_control, 
                  "Second Stage" = reg_second_stage, 
                  "2SLS" = reg_iv), 
             output = "kableExtra",
             gof_map = c("nobs"),
             coef_rename = c("fit_mil_aid" = "mil_aid"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 4), monospace = TRUE)
```

The output shows that the coefficient of `mil_aid` in the 2SLS model is the same as the one we calculated manually in the second stage (–0.494), which confirms that our specification is correct. However, the standard error is slightly larger (0.027 vs. 0.021), because `feols()` correctly incorporates the uncertainty from the first-stage estimation—a crucial aspect often missed in manual two-step procedures.

### Conclusion

IV regression is a valuable method for estimating causal effects in the presence of endogeneity—particularly when unobserved variables influence both the explanatory and the dependent variable. In situations where randomized experiments are not feasible and omitted variable bias threatens the validity of standard regression estimates, IV methods offer a way to recover credible causal relationships. However, this method works well only if we can find a valid instrument: it needs to be relevant, meaning it has a strong connection to the variable we are interested in, and exogenous, meaning it only influences the outcome through that variable (Stock & Watson, 2015).

This exercise has illustrated the logic and implementation of IV regression using simulated data. It also allowed us to observe the consequences of endogeneity and the conditions under which IV estimation can resolve it. By comparing simple OLS, controlled regressions, and IV estimators, we saw how omitted variable bias arises and how it can be addressed when a suitable instrument is available. The example also highlighted the practical advantage of using the `feols()` function for efficient and correct estimation of two-stage least squares models within a single step.

In the next exercise, we will use what we've learned and apply the instrumental variable approach to our actual data on police militarization, helping us see how useful it is for applied research.

<br/>

## Exercise 3.4 -- Instrumental Variables and Military Aid: Causal Estimates in Panel Data

In the previous exercise, we introduced the core logic behind instrumental variable (IV) regression using a simulated dataset. You learned how IV estimation can correct for endogeneity by isolating exogenous variation through a valid instrument. Now, we apply this method to our actual dataset on U.S. police militarization to address a key challenge: the non-random allocation of military aid.

Our central concern remains the same—estimating the causal impact of military aid on crime rates. However, simple OLS or even fixed-effects models may still suffer from omitted variable bias if some relevant but unobserved factors—such as long-term crime experience or institutional characteristics—affect both the level of aid received and the crime rate. This is where IV regression becomes essential.

Before proceeding with our analysis, we need to reload our dataset. Press `check` to run the chunk below.

```{r "13_1"}
mil <- readRDS("data/militarization.RDS")
```

### Selecting the Instrumental Variable

In this exercise, we apply the instrumental variable strategy developed in the previous section to our real-world panel dataset mil. Our goal remains the same: to estimate the causal effect of military aid on crime while addressing the problem of endogeneity, which arises when aid is not randomly assigned to counties but instead responds to underlying crime conditions.

As before, we use U.S. military expenditure as an instrumental variable. However, because this variable contains only six distinct values (one per year), it offers no variation across counties in a given year. If used directly, it would assign the same predicted value of aid to every county for a given year—making it ineffective in a panel context with fixed effects. To address this, we construct a composite instrument that introduces both temporal and cross-sectional variation.

Specifically, we interact lagged military expenditure with each county’s aid receipt probability, defined as the share of years (2006–2012) in which the county received any aid through the 1033 Program. The lag reflects the delay between military spending and the availability of surplus equipment—capturing the time it takes for new military goods to be decommissioned and redistributed.

This interaction strategy introduces meaningful variation along two key dimensions:

* **Extensive margin**: whether a county typically receives aid. Which is reflected in the aid receipt probability, which varies across counties.

* **Intensive margin**: the overall supply of available equipment, proxied by lagged military spending, which varies across years.

By combining both elements, the instrument captures realistic variation in aid flows over time and across counties—essential for identifying causal effects in a panel setting.

#### Assessing Instrument Validity

To serve as a valid instrument in a 2SLS framework, our constructed variable must satisfy two core assumptions:

* **Relevance**: It must be strongly correlated with the endogenous regressor—here, the log of lagged military aid value. This condition is plausible: military expenditure determines the supply of surplus equipment, and counties with a history of receiving aid are more likely to benefit when surplus is high. We will test this assumption statistically using the first-stage F-statistic; a value above 10 suggests a strong instrument.

* **Exogeneity**: The instrument must affect crime only through military aid and not via any alternative channel. While this condition cannot be formally tested, it can be justified substantively. U.S. military spending is determined by national defense policy—not local crime levels—and the Posse Comitatus Act prevents direct military involvement in domestic policing. However, we must recognize a residual risk: systematic responses to broader unrest or perceived crime waves could compromise exogeneity. However, the lag structure and inclusion of state-year fixed effects mitigate this concern by absorbing such time-varying shocks.

A further issue concerns the potential mechanical correlation between the aid receipt probability and actual aid received. Since we have shown in exercise 2.3 that counties that frequently receive aid also tend to receive more aid, this phenomenon could inflate the first-stage estimate. To address this, we include county fixed effects, which absorb all time-invariant county-level traits—including aid propensity—ensuring that the identifying variation comes from changes in military expenditure over time, rather than persistent county characteristics.

#### Conceptual Parallels to DiD

This instrument design conceptually mirrors a difference-in-differences (DiD) strategy, allowing us to compare counties with varying propensities to receive aid in years with high versus low national military spending. As noted by Nunn and Qian (2014), this type of interaction-based instrument leverages both cross-sectional and temporal variation to strengthen identification. Unlike traditional DiD designs, however, our treatment—military aid—is continuous, allowing us to estimate nuanced effects of varying aid intensity.

In summary, this IV construction is tailored to the structure of our panel dataset and grounded in a plausible causal logic. It enables us to isolate exogenous variation in military aid and move closer to identifying its true impact on crime rates—despite the limitations of observational data.


### Running the first stage

Now we are ready to estimate the first stage of our instrumental variable regression. This step allows us to evaluate whether our instrument—`milex_iv`, the interaction of lagged military expenditure and county-specific aid receipt probability—is sufficiently predictive of military aid. In statistical terms, we are testing the relevance condition for a valid instrument. 

**Task:** Fill in the gaps to estimate the first-stage regression as described above and save the result in `mod_first_stage`. 

```{r "13_2"}
mod_first_stage <- feols(___ ~ ___ + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = mil, cluster = ~ state, ssc = ssc(adj=FALSE, cluster.adj = FALSE, t.df = "conventional"))
modelsummary(list("First Stage" = mod_first_stage), 
             output = "kableExtra",
             gof_map = "nobs",
             coef_map = c("milex_iv" = "milex_iv"),
             stars = TRUE
             ) |>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "lag_log_aid_value" = 1), monospace = TRUE)
```

The regression output shows a strong and statistically significant effect of `milex_iv` on `lag_log_aid_value`, with a coefficient of 17.589 significant at the 0.1% level. The coefficient of 17.589 is significant at the 0.1% level, providing empirical support for the instrument’s relevance. However, statistical significance alone is not sufficient—what matters is whether the instrument is strong enough to avoid bias in the second-stage estimates.

#### Testing for Weak Instruments

In IV settings, weak instruments can lead to biased and unreliable estimates, especially in finite samples. To evaluate instrument strength more formally, we use the Kleinbergen-Paap (KP) F-statistic (Kleibergen & Paap, 2006), a robust diagnostic designed for clustered and heteroskedastic error structures. It generalizes the classic first-stage F-test and remains valid in panel settings with fixed effects.

The KP F-statistic tests the null hypothesis that the instrument is weak. A commonly used threshold is

* **KP F > 10** → the instrument is considered strong.

* **KP F < 10** → potential weak instrument problem, IV estimates may be biased.

Usually, the KP F-statistic would be computed automatically using the `fitstat()` function from the `fixest` package. However, this functionality is currently not working, as documented in an open issue on the GitHub page of `fixest`. Since the bug has not yet been resolved, we manually calculate the KP F-statistic as a workaround.

In our case—with one endogenous regressor and one instrument—the computation is straightforward. We divide the estimated coefficient of the instrument by its cluster-robust standard error to obtain the t-statistic, and then square that value:

$$
F_{\text{KP}} = \left( \frac{\hat{\beta}}{SE(\hat{\beta})} \right)^2
$$

This approach yields the same result as the method used internally in a custom `glance_custom.fixest()` function, which we use later to display KP F-statistics in modelsummary() regression tables. Press `check` to run the chunk below and calculate the KP F-statistic for our first stage.


```{r "13_3"}
t_stat <- mod_first_stage$coefficients[1] / mod_first_stage$se[1]

kp_f_stat <- data.frame("kp_f_stat" = unname(round(t_stat^2, digits = 3)))

kable(kp_f_stat, col.names = "Kleinbergen Paap F-Statistic")

```

</br>

With a value of 49.078, the KP F-statistic exceeds the conventional threshold of 10 by a wide margin. This finding confirms that `milex_iv` is a strong instrument, and the second-stage IV estimation can proceed without concern for weak-instrument bias.

#### Visualizing the First Stage

To further validate the strength and plausibility of the instrument visually, we generate a binned scatter plot with a fitted regression line. Before doing so, we residualize both the instrument and the endogenous variable with respect to all controls and fixed effects. This visual diagnostic helps assess linearity and verify overall model fit.


```{r "13_4"}

df_fig <- mil

df_fig$milex_iv_res <- residuals(feols(milex_iv ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = df_fig, cluster = ~ state)) + mean(df_fig$milex_iv)

df_fig$ltotal_cost_res <- residuals(feols(lag_log_aid_value ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = df_fig, cluster = ~ state)) + mean(df_fig$lag_log_aid_value)

df_fig$ltotal_cost_res_pred <- predict(feols(ltotal_cost_res ~ milex_iv_res, data = df_fig))

df_fig$bins_a <- cut(df_fig$milex_iv_res,
                      breaks = quantile(df_fig$milex_iv_res, probs = seq(0,1, by = 0.05)),
                      labels = 1:20,
                      include.lowest = TRUE,
                      right = FALSE)


df_fig |>
  group_by(bins_a) |>
  summarise(x = mean(milex_iv_res),
            y = mean(ltotal_cost_res)) |>
  ggplot(mapping = aes(x = x, y = y)) +
  geom_point(color = "darkblue", size = 2) + 
  geom_line(data = df_fig, mapping = aes(x = milex_iv_res, y = ltotal_cost_res_pred, color = "darkred")) +
  labs(title = "Total military aid (in log) and military expenditure IV: The first stage",
       x = "Military expediture IV",
       y = "Aid value") +
  scale_x_continuous(limits = c(4.275, 4.382),
                     breaks = c(4.28, 4.3, 4.32, 4.34, 4.36, 4.38)) +
  scale_y_continuous(limits = c(1.5, 3.5),
                     breaks = c(1.5, 2, 2.5, 3, 3.5)) +
  theme_bw() +
  theme(legend.position = "none")

```


The plot displays the first-stage regression line alongside a binned scatter plot, which provides a simplified visual summary of the relationship between our instrument and the endogenous variable. In a binned scatter plot, the raw data points are grouped into a predefined number of equally sized bins—in our case, 20—and the average value within each bin is plotted. This technique helps reveal underlying patterns in the data without cluttering the graph, which is especially useful when working with large datasets.

As the figure shows, the binned averages lie close to the fitted line throughout the observed range, suggesting a strong and stable linear relationship. There are no clear signs of non-linearity or influential outliers. While binning smooths over some variation, it provides a clear visual confirmation of the statistical results.

In addition to the binned scatter plot introduced earlier, we now present a standard scatter plot to visualize the original distribution of the data points. While binning helps to simplify complex datasets and highlight broad trends, it can obscure variation and outliers. This unbinned version allows us to assess whether the underlying data support the same interpretation as the binned plot. Just press `check` to run the chunk below and display the figure.

```{r "13_5"}
df_fig |>
  ggplot(mapping = aes(x = milex_iv_res, y = ltotal_cost_res)) +
  geom_point(color = "darkblue", size = 1, alpha = 0.2) + 
  geom_line(data = df_fig, mapping = aes(x = milex_iv_res, y = ltotal_cost_res_pred, color = "darkred")) +
  labs(title = "Total military aid (in log) and military expenditure IV: The first stage",
       x = "Military expediture IV",
       y = "Aid value") +
  theme_bw() +
  theme(legend.position = "none")
```

The plot reveals that the slope of the regression line appears much flatter than in the binned version. This impression arises because the range of the y-axis is considerably wider, while the x-axis remains relatively narrow. This reduces the visual impact of the positive relationship. Moreover, the tight fit suggested by the binned plot is not as evident here—individual data points are widely dispersed around the fitted line. This comparison highlights the importance of checking the raw, unaggregated data in addition to simplified visualizations. While binning improves readability, it can sometimes create an overly optimistic impression of model fit or strength of association. Reviewing both versions allows for a more balanced and transparent interpretation of the first-stage relationship.

Overall, the regression results, KP F-statistic, and visual checks strongly suggest that milex_iv is an important and effective instrument. We are now ready to move on to the second stage of our IV regression to estimate the causal effect of military aid on crime.

### Running the second stage

We will now present the regression tables for the fixed effects model, which includes clustered standard errors from exercise 3.2, alongside the IV model for comparison. If military aid is indeed endogenous, the IV estimate should differ from the fixed effects estimate, providing a more accurate measure of the causal effect. 

**Task:** Fill in the gaps to estimate the 2SLS regression.

```{r "13_6"}
mod_iv <- feols(___ ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction | ___ ~ ___, data = mil, cluster = ~ state, ssc = ssc(adj=FALSE, cluster.adj = FALSE, t.df = "conventional"))

mod_cluster <-  feols(crime_rate ~ lag_log_aid_value + log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = mil, cluster = ~ state, ssc = ssc(adj=FALSE, cluster.adj = FALSE, t.df = "conventional"))


modelsummary(list("Fixed Effects" = mod_cluster, "IV" = mod_iv),
             coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value", 
                          "lag_log_aid_value" = "lag_log_aid_value"),
             gof_map = list(
                            list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
                            list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
                            ),
             stars = TRUE,
             output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15) |>
  add_header_above(c(" " = 1, "Dependent Variable: crime_rate" = 2), monospace = TRUE)

```


As the regression results show, the coefficient for `lag_log_aid_value` changes substantially between models: it is small, positive, and statistically insignificant in the fixed effects regression but it turns large, negative, and highly significant (p < 0.001) in the instrumental variable specification. This difference strongly indicates that the original fixed effects model had problems with endogeneity—probably because of unobserved variables-which is effectively handled by the IV approach.


Quiz: If we assume causality, how would we interpret the coefficient of the IV regression of -59.293?

(1) 10$ more aid leads to approximately 5.9 more crimes.
(2) 10% more aid leads to approximately 59 less crimes per 100,000 population.
(3) 10$ more aid leads to approximately 5.9% less crimes.
(4) 10% more aid leads to approximately 5.9 less crimes per 100,000 population.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("coeffiecient_interpretation_2")
```

<br/>

#### Visualizing the Second Stage

To validate the fit of our second stage visually, we again generate a binned scatter plot with a fitted regression line. We again use residualized values of both `crime_rate` and the fitted values `lag_log_aid_value` from the first stage considering all controls and fixed effects. This visual diagnostic helps assess linearity and verify overall model fit.

```{r "13_7"}
df_fig$iv_pred <- predict(mod_first_stage)

df_fig$iv_pred_res <- residuals(feols(iv_pred ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = df_fig, cluster = ~ state)) + mean(df_fig$iv_pred)

df_fig$crime_res <- residuals(feols(crime_rate ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction, data = df_fig, cluster = ~ state)) + mean(df_fig$crime_rate)

df_fig$crime_res_pred <- predict(feols(crime_res ~ iv_pred_res, data = df_fig))

df_fig$bins_b <- cut(df_fig$iv_pred_res,
                      breaks = quantile(df_fig$iv_pred_res, probs = seq(0,1, by = 0.05)),
                      labels = 1:20,
                      include.lowest = TRUE,
                      right = FALSE)

df_fig |>
  group_by(bins_b) |>
  summarise(x = mean(iv_pred_res),
            y = mean(crime_res)) |>
  ggplot(mapping = aes(x = x, y = y)) +
  geom_point(color = "darkblue") + 
  geom_line(data = df_fig, mapping = aes(x= iv_pred_res, y = crime_res_pred, color = "darkred")) +
  labs(title = "Total military aid (in log) and crime rate (in 100,000): The second stage",
       x = "Predicted aid value",
       y = "Crime rate") +
  scale_x_continuous(limits = c(1.5, 3.5),
                     breaks = c(1.5, 2, 2.5, 3, 3.5)) +
  scale_y_continuous(limits = c(2400, 2555),
                     breaks = c(2400, 2450, 2500, 2550)) +
  theme_bw() +
  theme(legend.position = "none")
```

Similar to the first-stage plot, the data points lie relatively close to the regression line, suggesting a good fit of the model and a consistent linear pattern across the range of predicted aid values.

As for the first-stage regression, we also present a standard scatter plot for the second stage to examine the raw distribution of the data points without the smoothing effect of binning. This feature helps us to validate the robustness of our results by visually inspecting the fit and overall pattern in the non-aggregated data.

Simply press `check` to run the chunk and display the plot.

```{r "13_8"}
df_fig |>
  ggplot(mapping = aes(x = iv_pred_res, y = crime_res)) +
  geom_point(color = "darkblue", size = 1, alpha = 0.2) + 
  geom_line(data = df_fig, mapping = aes(x= iv_pred_res, y = crime_res_pred, color = "darkred")) +
  labs(title = "Total military aid (in log) and crime rate (in 100,000): The second stage",
       x = "Predicted aid value",
       y = "Crime rate") +
  scale_x_continuous(limits = c(0, 5)) +
  scale_y_continuous(limits = c(-2500, 8000)) +
  theme_bw() +
  theme(legend.position = "none")
```

Compared to the first-stage plot, the slope of the regression line appears noticeably flatter, which again is due to the wider scale of the y-axis, while the x-axis range remains relatively constrained. Despite this visual effect, the data still cluster closely around the fitted line, suggesting a reasonably good fit even in the raw data. Although the alignment is not as tight as in the binned version, a clear linear trend is visible, supporting the consistency and reliability of the second-stage regression. Including both versions—binned and raw—helps to verify that our estimated relationship between predicted aid and crime rate is not merely an artifact of data transformation but is also evident in the original dataset.

Instrumental variable regression is a powerful tool for addressing endogeneity, allowing us to estimate causal effects even when confounders are unobserved or measurement error is present. In this analysis, we use lagged U.S. military expenditure—interacted with county aid propensity—as an instrument for military aid. This strategy helps to isolate variation that is plausibly exogenous to local crime dynamics.

While fixed effects regression accounts for time-invariant unobserved heterogeneity—such as geography or local policing institutions—it cannot eliminate bias from time-varying unobserved factors. The IV approach complements fixed effects by leveraging external variation in military aid, which sharpens identification and enhances the credibility of the estimated effect.

 The substantial shift in the coefficient between the fixed effects and IV models illustrates the value of checking for endogeneity. Without the IV approach, we may have incorrectly concluded that military aid has no effect—or even a positive effect—on crime.

In conclusion, this IV analysis provides stronger evidence that military aid has a crime-reducing effect and demonstrates how econometric tools can lead to more rigorous, policy-relevant conclusions. Still, the validity of these findings rests on the assumptions behind the IV strategy—most importantly, that the instrument affects crime only through military aid (Angrist & Pischke, 2008). This assumption must always be critically evaluated when interpreting causal results from observational data.

<br/>

## Exercise 4 -- Mechanisms and Broader Effects

In the previous exercises, we gradually developed an empirical strategy to estimate the causal effect of military aid on crime. Building on this foundation, the present exercise takes a more differentiated perspective by examining whether and how this effect varies across different categories of crime.

Beyond measuring overall crime reduction, we are also interested in the mechanisms through which military aid might influence public safety. Does militarization deter crime by making police appear more threatening? Or does it enhance police effectiveness and increase the likelihood of apprehending offenders?

To explore these questions, we extend our analysis in three directions:

* Disaggregated crime outcomes – to assess whether the effect of military aid differs across specific crime types (e.g. robbery, larceny, assault);

* Law enforcement outcomes – like arrest rates and other indicators of police activity, which may shed light on the operational consequences of militarization.

* Disaggrrgated military aid - to assess which components of military aid drive the effect on crime.

Through this broader lens, we aim to better understand not only whether military aid reduces crime, but also how and why it does so.

### Structure

4.1 — Crime Types: Disaggregating the Impact of Military Aid

4.2 — Arrest Rates and the Mechanisms of Deterrence

4.3 — Beyond Crime: Organizational and Behavioral Effects of Militarization

4.4 — Supply Categories: Effectiveness of Military Aid

<br/>

## Exercise 4.1 -- Crime Types: Disaggregating the Impact of Military Aid

Having established an overall negative effect of military aid on crime in the previous section, we now turn to a more nuanced question: Do the effects of militarized policing vary across different types of criminal offenses?

In this exercise, we investigate potential heterogeneous effects of military aid (`lag_log_aid_value`) by estimating separate regression models for specific crime categories. In addition to overall crime, we examine outcomes for murder, robbery, assault, burglary, larceny, and vehicle theft—each analyzed both in absolute terms (crime rates per 100,000 population) and, where applicable, in relative terms (e.g. proportional shifts across categories).

This disaggregated analysis allows us to test whether police militarization deters certain types of crime more effectively than others. For example, one might expect military aid to have a stronger deterrent effect on violent offenses (such as assault or robbery), while exerting less influence on property crimes (like larceny or vehicle theft). By systematically comparing estimated coefficients across categories, we can identify such patterns and assess their statistical and substantive significance.

The results of this exercise will help clarify the policy implications of our findings. If effects differ across crime types, the results could inform a more targeted discussion on when and where military-grade equipment improves public safety—or potentially exacerbates tensions.

Before running the models, we need to reload the dataset to ensure all variables are available and up-to-date. Simply press `check` to execute the chunk below.

```{r "15_1"}

mil <- readRDS("data/militarization.RDS")

```


We begin this analysis by estimating a series of regression models that use crime rates (per 100,000 population) as outcome variables across various crime categories. This approach allows us to quantify the absolute impact of military aid on specific types of offenses and to explore whether some crimes are more sensitive to changes in militarized policing than others.

By comparing the magnitude, direction, and statistical significance of these coefficients, we aim to uncover heterogeneous effects—for example, whether military aid is more effective at reducing violent crimes like robbery or assault than property-related offenses such as burglary or larceny.

To provide a comprehensive reference point, the output table also includes estimates from three relevant benchmark models:

* the baseline fixed effects regression with clustered standard errors (introduced in exercise 3.2),

* the first-stage regression from the instrumental variable approach,

* and the regression on total crime as an aggregated outcome.

Together, these comparisons help contextualize the results and show how the estimated impact of military aid varies across both specifications and crime categories.

Once you're ready, press `check` to execute the chunk and run the full analysis.

```{r "15_2"}

results_1_1 <- list()

# OLS
results_1_1[["OLS"]] <- feols(
  crime_rate ~ lag_log_aid_value + poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction,
  data = mil, cluster = ~ state
)

# First stage
results_1_1[["Fist stage"]] <- feols(
  lag_log_aid_value ~ milex_iv + poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction,
  data = mil, cluster = ~ state
)

# Crime
results_1_1[["Crime"]] <- feols(
  crime_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Murder
results_1_1[["Murder"]] <- feols(
  murder_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Robbery
results_1_1[["Robbery"]] <- feols(
  robbery_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Assault
results_1_1[["Assault"]] <- feols(
  assault_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Burglary
results_1_1[["Burglary"]] <- feols(
  burglary_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Larceny
results_1_1[["Larceny"]] <- feols(
  larceny_rate ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Vehicle Theft
results_1_1[["Vehicle Theft"]] <- feols(
  vehicle_theft_rate ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)



modelsummary(
  models = results_1_1,
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value", 
               "lag_log_aid_value" = "lag_log_aid_value",
               "milex_iv" = "milex_iv"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)

```



Quiz: For which crime type does military aid show the most substantial negative effect?

(1) Murder
(2) Robbery
(3) Larceny
(4) Assault

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("crime_type_effect")
```

<br/>

The regression results reveal clear heterogeneity in the effect of military aid across different types of crime. While the overall crime rate declines significantly in response to increased aid (−59.293, p < 0.001), the strength and significance of this effect vary considerably by offense type.

For murder, the estimated coefficient (−0.063) is small and statistically insignificant, suggesting no meaningful relationship between military aid and homicide rates. This is plausible, as homicides are often driven by complex, deeply rooted social and interpersonal factors—such as domestic disputes, gang violence, or long-term community disinvestment—that are unlikely to be deterred by the presence of military equipment (Butts et al., 2015).

A similar pattern is observed for burglary, where the estimated effect (−8.750) is again statistically insignificant due to a relatively large standard error. Burglary is typically a low-visibility, non-confrontational crime committed in private spaces. As such, it may be less responsive to increased militarization, which tends to operate in visible, public-facing contexts (Wright & Decker, 1994).

In contrast, the results for robbery, assault, larceny, and vehicle theft show statistically significant negative effects, with coefficients ranging from −5.3 to −27.4. Notably, the effect on larceny is both the largest in magnitude and highly significant (p < 0.001), making it the category most affected by military aid. These crimes often occur in public spaces and involve a greater chance of police intervention, making them more sensitive to the deterrent presence of militarized policing. The strong and consistent significance levels across these categories suggest a plausible deterrence mechanism: the visibility and perceived threat of militarized enforcement likely discourage opportunistic or confrontational behavior in public areas.

That said, we must interpret these absolute effect sizes with caution. Crime categories differ vastly in their baseline frequencies: for example, the average U.S. murder rate is around 3.4 per 100,000 people, whereas larceny exceeds 1,500 per 100,000. This disparity means that the same absolute change (e.g. −10) may represent a substantial drop in one category and a negligible shift in another. In other words, absolute effects are not directly comparable across crime types due to scale differences.

To address this issue and enable a more meaningful cross-category comparison, we now estimate the same models using logarithmized crime rates. This log-log specification allows us to interpret the coefficients as percentage changes, which gives us greater clarity about the relative impact of military aid across offenses with very different base rates.

> *Note:* These log-transformed models are not part of the original analysis presented in the paper. They have been added here as an extension to improve comparability and deepen the interpretation of heterogeneous effects across crime categories.

Just press `check` to run the next chunk and repeat the analysis using log-transformed outcome variables.


```{r "15_3"}

results_1_2 <- list()

# Crime
results_1_2[["Crime"]] <- feols(
  log(crime_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Murder
results_1_2[["Murder"]] <- feols(
  log(murder_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Robbery
results_1_2[["Robbery"]] <- feols(
  log(robbery_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Assault
results_1_2[["Assault"]] <- feols(
  log(assault_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Burglary
results_1_2[["Burglary"]] <- feols(
  log(burglary_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Larceny
results_1_2[["Larceny"]] <- feols(
  log(larceny_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population + share_male +
    share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

# Vehicle Theft
results_1_2[["Vehicle Theft"]] <- feols(
  log(vehicle_theft_rate) ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years + share_25_29_years +
    share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil, cluster = ~ state,
  ssc = ssc(adj=FALSE, cluster.adj = FALSE)
)

modelsummary(
  models = results_1_2,
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)

```


While only the coefficient for robbery remains statistically significant at the 5% level, the log-log results reveal an important shift in the relative effect sizes across crime categories. Most notably, larceny, which previously exhibited the largest absolute effect, now shows the smallest proportional impact with a coefficient of just −0.006.

In contrast, the estimated effects for murder, assault, burglary, and vehicle theft all fall within a relatively narrow range of approximately −0.010 to −0.016. This pattern suggests that, when measured in relative terms, the influence of military aid on these offenses is more uniform than initially indicated by the absolute-rate models.

The most pronounced effect is observed for robbery, which now has the largest elasticity at −0.036. This means that a 10% increase in military aid is associated with a 0.36% reduction in robbery rates, holding other factors constant. One plausible explanation for this result lies in the low baseline frequency of robbery: because the log-log model captures proportional changes, even modest absolute reductions in robbery can yield amplified percentage effects—and thus larger coefficients.

These results highlight the significance of selecting the correct functional form when making comparisons across outcomes that have vastly different base rates. While the absolute-rate models emphasized public space crimes like larceny, the log-log specification highlights robbery as the most elastic to changes in military aid. This shift in interpretation reinforces the complementarity of both model types and illustrates how transformation choices can shape empirical conclusions.

### Cost and Benefit Analysis

Beyond statistical significance, it is essential to consider the practical relevance of our findings: Do the observed reductions in crime translate into meaningful social or economic benefits? And are these benefits sustainable?

A key limitation of our study is that it focuses exclusively on short-term effects, capturing changes in crime rates within a one-year window after military aid is allocated. As such, we cannot assess whether these impacts persist over time or fade as communities adjust to the presence of militarized policing. The long-term implications—including potential community alienation, changes in police-civilian relationships, or crime displacement effects—remain outside the scope of this analysis and should be explored in future research.

Nonetheless, even short-term deterrent effects can be economically consequential when viewed through the lens of cost-benefit analysis. Based on estimates from Heaton (2010), the average social cost per crime is substantial—around $67,277 for a robbery, $87,238 for a serious assault, and $13,096 for a burglary. These figures include both tangible costs (e.g. medical care, property loss) and intangible costs (e.g. pain, lost quality of life).

Applying these benchmarks, the authors calculate that a 10% increase in military aid (approximately $5,800 in value) leads to short-term reductions of approximately:

* 0.6 fewer robberies,

* 0.5 fewer assaults, and

* 0.9 fewer burglaries per county-year.

When monetized using Heaton’s crime cost estimates, these reductions yield an estimated total benefit of over $112,000, which far exceeds the value of the aid itself.From a fiscal perspective, the evidence suggests that the marginal return on investment in surplus military equipment is highly favorable—at least within the short-term evaluation window of this analysis.

Still, these figures should be interpreted with caution. Cost-effectiveness alone does not justify policy if longer-term harms outweigh the short-run gains.

### Alternative Explanations

While the estimated reductions in crime following military aid allocation appear statistically and economically meaningful, it is essential to critically assess whether these patterns truly reflect causal deterrent effects—or whether they might stem from alternative mechanisms.

The following considerations go beyond the scope of the original paper and represent additional reflections introduced in this problem set to deepen the interpretation of the findings.

One such explanation is crime displacement. Rather than reducing the overall incidence of crime, military aid may shift criminal activity to neighboring areas with lower police visibility. In this scenario, the observed decline in crime within militarized counties would not represent an actual reduction in criminal behavior but rather a spatial reallocation of offenses. This interpretation is consistent with lower arrest rates if criminals adjust their behavior to avoid areas with a visibly increased police presence (Leong, 2014).

Another potential explanation relates to community trust and reporting behavior. Heavily militarized policing may erode public confidence in law enforcement, particularly in marginalized communities. If residents feel intimidated or alienated by the visible presence of tactical equipment and armored vehicles, they may become less willing to report crimes or cooperate with police investigations. In such cases, the observed drop in recorded crime may not reflect a real decline in criminal activity, but rather a measurement artifact driven by reduced citizen engagement. This possibility is especially salient for crimes that rely heavily on victim or witness reporting, such as assault or domestic violence (Mummolo, 2018).

These scenarios highlight the importance of interpreting observed crime reductions with caution. Without additional evidence—such as victimization survey data, qualitative indicators of public trust, or spatial spillover analyses—it remains difficult to determine whether the measured effects reflect genuine improvements in public safety or unintended side effects of militarization.


### Conclusion

This analysis provides evidence that military aid is associated with reductions in several types of crime, particularly in street-level offenses such as robbery, larceny, and vehicle theft. These effects are both statistically and economically meaningful, suggesting that the visible presence of militarized policing may deter opportunistic crimes committed in public spaces.

At the same time, the results reveal important limitations and heterogeneity. For more severe or private crimes such as homicide and burglary, the effects of military aid are statistically insignificant, indicating that such offenses may be less responsive to short-term, visible deterrents. Additionally, the shift in relative effect sizes between the absolute and log-log models illustrates how functional form choices shape interpretation and suggests that one should exercise caution when drawing policy conclusions from aggregate effects.

Yet even if we accept that crime goes down in response to military aid, an important question remains: why does it go down? Is it because potential offenders are deterred by the mere presence of intimidating equipment, such as armored vehicles and assault rifles? Or does militarization actually make police forces more capable of apprehending offenders, thereby increasing the perceived risk of being caught and prosecuted?

To better understand the mechanism behind the observed effects, the next step in our analysis turns to arrest rates. By examining whether military aid increases the share of reported crimes that result in arrests, we aim to distinguish between two competing explanations: deterrence through threat versus deterrence through enforcement. This information will help clarify whether military-grade equipment primarily serves as a visible symbol of force—or whether it actively improves police effectiveness on the ground.

<br/>

## Exercise 4.2 -- Arrest Rates and the Mechanisms of Deterrence


### Introduction

In this exercise, we examine the potential mechanisms through which military aid may influence crime rates. While the previous analysis documented an overall reduction in crime, it remains unclear whether this decline is driven by an actual increase in law enforcement effectiveness or by a more symbolic deterrence effect.

We consider two plausible channels. First, military aid may create a visual deterrence effect, whereby the intimidating presence of militarized police discourages individuals from engaging in criminal behavior—even if actual police effectiveness remains unchanged. Second, military aid may enhance enforcement capacity by equipping officers with tools and technologies that increase the probability of detecting and arresting offenders. If so, the reduction in crime would reflect not just fear of being caught, but a real increase in the likelihood of apprehension.

To empirically distinguish between these two channels, we analyze arrest rates across crime categories. An increase in arrest rates following military aid allocation would suggest that improved policing efficiency is a central driver of the observed crime reduction. Conversely, if arrest rates remain unchanged or decrease while crime declines, this would point toward deterrence operating primarily through the perceived threat of force rather than through enhanced enforcement.

Before we begin our analysis, we first need to load the relevant data. Simply press `check` to run the code chunk below.

```{r "16_1"}

mil <- readRDS("data/militarization.RDS")

```


The *mil* dataset includes detailed information on arrest rates per 100,000 inhabitants for each crime category. In this part of the analysis, we shift our focus from crime occurrence to law enforcement outcomes, estimating regression models that use arrest rates—rather than crime rates—as the dependent variable.

We use the same model setup as before, with fixed effects and the instrumental variable approach, to ensure consistency and enable a direct comparison between effects on crime incidence and on arrests. This method allows us to assess whether military aid not only deters crime but also improves the capacity of police forces to apprehend offenders.

The following code executes these models across crime categories and provides the results for interpretation. Just press `check` to run it.

```{r "16_2"}
results_2_1 <- list()

results_2_1[["Murder"]] <- feols(
  arrest_rate_murder ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_1[["Robbery"]] <- feols(
  arrest_rate_robbery ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_1[["Assault"]] <- feols(
  arrest_rate_assault ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_1[["Burglary"]] <- feols(
  arrest_rate_burglary ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_1[["Larceny"]] <- feols(
  arrest_rate_larceny ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_1[["Vehicle Theft"]] <- feols(
  arrest_rate_vehicle_theft ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

modelsummary(
  models = results_2_1,
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)

```


The regression results show that the coefficients for military aid are negative across all crime categories, indicating that higher levels of aid are associated with lower arrest rates per 100,000 inhabitants. This pattern is consistent across offenses and statistically significant for burglary and vehicle theft, with suggestive evidence for robbery and larceny. While the effects for assault and larceny do not reach conventional significance thresholds, the consistent direction of the coefficients supports the hypothesis that military aid may exert a general deterrent effect.

Two main mechanisms could explain this observed reduction in arrests:

1. **Psychological deterrence through visible presence**:
   The deployment of military-style equipment can project an image of heightened authority and force. This visible militarization may discourage potential offenders from engaging in criminal activity due to the perceived risk of a more immediate or forceful response. In this scenario, crime is prevented before it occurs—leading to fewer offenses and, correspondingly, fewer arrests.

2. **Operational deterrence via enhanced police capacity**:
   Alternatively, military aid may improve law enforcement efficiency by supplying better vehicles, communication tools, and surveillance technologies. While such improvements might be expected to result in more arrests, it is also possible that enhanced capabilities enable police to prevent crime more effectively, thereby reducing both incidents and enforcement actions.

Although both mechanisms are plausible, our current results do not allow us to distinguish clearly between them. The observed reduction in arrest rates could reflect either fewer crimes being committed due to increased deterrence or fewer arrests being necessary because of improved preemptive capabilities.



Quiz: Which of the following best describes the two main ways military aid might deter crime?

(1) Fear of military equipment and improved police effectiveness.
(2) Increased police salaries and longer patrol hours.
(3) Community engagement programs and educational campaigns.
(4) Randomized policing strategies and harsher sentencing laws.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("deterrence_mechanisms")
```

<br/>

Crucially, we cannot yet determine whether relative arrest probabilities—that is, the likelihood of an arrest given a crime—have increased. A reduction in absolute arrest rates does not necessarily imply reduced police effectiveness; it may instead result from a drop in crime incidence, leaving fewer opportunities for arrests.

To clarify this distinction, the next step in our analysis is to examine **relative arrest rates**, calculated as the number of arrests per reported crime. This measurement will allow us to test whether military aid affects the efficiency of law enforcement—by increasing the share of crimes that lead to arrests—or whether its primary role is to deter crime through visual intimidation.

> *Note:* This analysis of relative arrest rates does **not** appear in the original paper and has been added as an **independent methodological extension** to deepen the interpretation of the underlying mechanisms.


**Task:** Complete the code by filling in the blanks to create new variables in the `mil` dataset that represent the number of arrests per reported crime for each crime category.

```{r "16_3"}
mil <- mil |>
  mutate(murder_arrests_per_crime = ___/murder_rate,
         robbery_arrests_per_crime = ___/robbery_rate,
         assault_arrests_per_crime = ___/assault_rate,
         burglary_arrests_per_crime = ___/burglary_rate,
         larceny_arrests_per_crime = ___/larceny_rate,
         vehicle_theft_arrests_per_crime = ___/vehicle_theft_rate)


results_2_2 <- list()

results_2_2[["Murder"]] <- feols(
  murder_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_2[["Robbery"]] <- feols(
  robbery_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_2[["Assault"]] <- feols(
  assault_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_2[["Burglary"]] <- feols(
  burglary_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_2[["Larceny"]] <- feols(
  larceny_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_2_2[["Vehicle Theft"]] <- feols(
  vehicle_theft_arrests_per_crime ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

modelsummary(
  models = results_2_2,
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)
```


The regression results for relative arrest rates—that is, the number of arrests per reported crime—show consistently small and statistically insignificant effects across all crime categories. The only exception is murder, where the coefficient of −0.070 reaches significance at the 5% level. This estimate suggests a slight negative relationship, although the effect is modest in size and should be interpreted with caution. The lower number of observations in these regressions is due to the fact that some counties did not report any crimes in specific categories during the observed period. We necessarily exclude these cases from the analysis because we cannot compute relative arrest rates in the absence of reported offenses. Despite the reduced sample size, the Kleibergen-Paap F-statistics remain above 40 for all models, indicating that the instrumental variable retains sufficient strength even in this reduced dataset. This lends credibility to the identification strategy and supports the robustness of the results.


Quiz: If we assume a causal relationship, what would the coefficient for murder of -0.070 mean? If we assume that every murder is committed by one single person.

(1) A 1% increase in military aid leads to a 7 percent lower arrest probability for the murder.
(2) A 100% increase in military aid leads to a 7 percentage point lower arrest probability for the murder.
(3) A 10% increase in military aid leads to a 7 percent lower arrest probability for the murder.
(4) A 100% increase in military aid leads to a 7 percent lower arrest probability for the murder.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("interpretation_coefficient_relative_arrest_rate")
```

<br/>

Taken together, these findings suggest that military aid does not significantly increase law enforcement efficiency, at least as measured by the likelihood of an arrest following a reported crime. The result challenges the notion that militarized equipment improves the operational performance of police forces in solving crimes. Instead, the lack of effect on relative arrest rates points toward a deterrence-based mechanism: potential offenders may be discouraged from committing crimes not because they expect to be caught, but because the visible presence of militarized policing acts as a psychological deterrent.

In this interpretation, the previously observed reduction in crime and absolute arrest rates is not driven by higher arrest success per crime but rather by fewer crimes being committed in the first place. The lower arrest counts are thus a downstream effect of deterrence—not improved efficiency.

Overall, the evidence suggests that military aid exerts its influence symbolically rather than operationally. It appears to affect behavior through the perception of risk rather than through measurable gains in police effectiveness. While this approach still qualifies as a form of crime prevention, it raises important questions about the sustainability and broader social consequences of relying on militarized deterrence.


### Conclusion

The analysis presented in this exercise suggests that military aid is associated with reductions in absolute arrest rates across several crime categories. However, since we observe no significant effect on relative arrest rates—that is, arrests per reported crime—for most categories, we can conclude that the reduction in arrests is not driven by improved police effectiveness. Instead, it is more plausibly explained by lower crime incidence, consistent with a deterrence-based mechanism.

In other words, military aid appears to reduce crime not because it makes police better at apprehending offenders, but because its visible presence discourages criminal activity from occurring in the first place. This distinction is crucial for understanding the actual role militarization plays in shaping law enforcement outcomes.

Up to this point, our analysis has focused exclusively on crime-related measures. However, military aid may also influence broader aspects of police performance—such as resource deployment, use-of-force incidents, or community engagement. We now turn to the analysis of alternative police outcome variables to gain a more comprehensive picture of how militarization affects policing beyond crime statistics.

<br/>


## Exercise 4.3 -- Beyond Crime: Organizational and Behavioral Effects of Militarization

In this exercise, we investigate how military aid influences broader aspects of policing beyond crime and arrest rates—specifically, operational and organizational outcomes that may shape how police departments function and how offenders perceive the risk of apprehension. This analysis helps us assess whether militarization affects not only crime directly but also the internal behavior and structure of law enforcement agencies.

By examining additional police-related outcomes, we gain a more nuanced understanding of the mechanisms behind the observed deterrent effects. In particular, we aim to clarify whether military aid leads to tangible changes in how agencies allocate resources, deploy personnel, or interact with the public. These institutional shifts are essential for interpreting the long-term policy implications of police militarization, including its potential effects on public trust, accountability, and the strategic priorities of law enforcement.

To conduct this analysis, we use both the *militarization* and *police* datasets. While *militarization* contains county-level data used in earlier exercises, *police* offers agency-level information, including many of the same controls but with additional detail about the police departments themselves. This richer data structure allows us to examine the effects of military aid on outcomes that are more directly tied to agency operations, such as staffing levels, equipment use, or citizen complaints.

Before we begin, we need to load the relevant data. Simply press check to run the code chunk below and start the analysis.

```{r "17_1"}

mil <- readRDS("data/militarization.RDS")

pol <- readRDS("data/police.RDS")

```


In the following analysis, we examine how military aid affects key dimensions of police staffing and operational behavior at the agency level. Specifically, we assess whether changes in the number of sworn officers and civilian employees per 100,000 inhabitants can be observed following the receipt of military aid. In addition, we analyze effects on the officer-to-employee ratio, as well as on operational outcomes such as the number of service calls, reported assaults or injuries to officers in the line of duty, and the number of offenders killed by police.

These regressions are based on the pol dataset, which provides agency-level panel data. To account for structural differences between law enforcement agencies—such as size, jurisdiction type, or baseline staffing—we use agency fixed effects rather than county fixed effects. This method allows us to estimate the within-agency effects of military aid over time and to isolate changes in behavior or capacity from time-invariant institutional characteristics.

To complement this analysis, we include an additional regression using the mil dataset to assess the relationship between military aid and the number of citizen complaints filed against law enforcement. This outcome captures a dimension often emphasized in critiques of the 1033 Program: the concern that militarized police departments may engage in more aggressive or disproportionate responses, potentially undermining public trust and accountability.

The following code runs the relevant models and displays the results for interpretation.


```{r "17_2"}
results_3 <- list()


results_3[["Officers"]] <- feols(
  officer_rate ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Civilian employees"]] <- feols(
  civilian_employees ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Officers to employees ratio"]] <- feols(
  officer_employee_ratio ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Calls"]] <- feols(
  calls ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Injuries"]] <- feols(
  injuries ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Civil disorder assaults"]] <- feols(
  civil_disorder_assaults ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Offenders killed"]] <- feols(
  offenders_killed ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | agency + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = pol,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

results_3[["Citizen Complaints"]] <- feols(
  log_citizen_complaints ~ poverty_rate + log_median_income + unempl_rate + log_population +
    share_male + share_black + share_15_19_years + share_20_24_years +
    share_25_29_years + share_30_34_years | county + state_year_interaction |
    lag_log_aid_value ~ milex_iv,
  data = mil,
  cluster = ~ state,
  ssc = ssc(adj = FALSE, cluster.adj = FALSE)
)

modelsummary(
  models = results_3,
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)

```


The regression results show that the coefficients for both sworn officers and civilian employees are negative and statistically significant—at the 10% and 5% levels, respectively. In contrast, the officer-to-employee ratio remains unchanged, with an insignificant coefficient close to zero (0.001). These findings indicate a potential substitution effect between labor and capital in the police production function: the provision of military equipment may increase the productivity of existing personnel, thereby reducing the need to expand staff size while maintaining a stable organizational structure. It is important to note, however, that the number of sworn officers is reported per 1,000 inhabitants, while the number of civilian employees is recorded as an absolute count. As a result, the two coefficients are not directly comparable in magnitude, and differences must be interpreted with caution.

In addition, military aid shows no significant effect on broader indicators of police activity or officer safety. Specifically, there is no measurable impact on the number of calls for service, incidents of officer injury or assault, or the number of offenders killed by police. These null results are particularly relevant in light of earlier concerns raised in Exercise 4.1, where we considered the possibility that militarization might reduce crime reports due to public fear or a breakdown in trust. The absence of a decline in call volume provides reassurance at this point and suggests that the observed reduction in crime is unlikely to be driven solely by reduced public engagement with law enforcement. In terms of instrument strength, the Kleibergen-Paap F-statistics for these regressions (except for “offenders killed”) exceed the conventional threshold of 10, indicating reasonably strong instruments. However, compared to earlier models using the mil dataset, instrument strength here is noticeably lower.

To explore the potential impact of military aid on citizen–police relations, we estimate an additional model using the mil dataset to assess its effect on the number of citizen complaints. The model addresses a central concern in the public debate around the 1033 Program: the risk that military-grade equipment may encourage more aggressive or disproportionate policing tactics. If this were the case, we would expect to see a positive and significant effect on the number of complaints.

However, our analysis reveals a small and statistically insignificant negative coefficient. While this could suggest the absence of harmful behavioral effects, the result must be interpreted with considerable caution. The number of observations with valid complaint data is limited to just 534, and the KP F-statistic for this model is only 2.322, well below the commonly accepted threshold of 10—indicating a potential weak instrument problem.

Moreover, as Harris et al. (2017) argue, the relationship between militarization and complaints may be more complex. Military-style equipment could both increase police aggression (leading to more complaints) and intimidate civilians (reducing their willingness to report misconduct). This dual possibility complicates interpretation and points out the need for more granular data and qualitative insights to understand how militarization affects public accountability and trust.

Taken together, these findings reinforce our earlier conclusion from Exercise 4.2: military aid contributes to lower crime rates primarily through a deterrence mechanism, rather than through improvements in arrest efficiency or aggressive enforcement behavior. Whether this deterrence arises from the symbolic presence of militarized policing or from unmeasured institutional changes remains an open question. Nonetheless, the results suggest that militarization operates more through behavioral influence than through a transformation of police operations or capacity.

<br/>


## Exercise 4.4 -- Supply Categories: Effectiveness of Military Aid

In the previous exercises, we estimated the overall effect of military aid on crime rates without differentiating between the specific types of equipment transferred to law enforcement agencies. However, treating all military aid as homogeneous may obscure important differences in effectiveness. Intuitively, the transfer of office furniture and the provision of an armored vehicle are unlikely to influence police operations—or criminal behavior—in the same way. Ignoring such distinctions could lead to misleading or incomplete conclusions about the actual effects of militarization.

To address this limitation, we now disaggregate military aid into four broad categories: weapons, vehicles, tactical gear, and other equipment. This distinction is both analytically and policy-relevant. Understanding which types of aid drive reductions in crime can help refine program design, improve cost-effectiveness, and mitigate potential harms.

This part of the analysis builds on the previous exercises by applying the same empirical strategy—instrumental variable estimation—to each aid category separately. By doing so, we aim to identify whether certain types of equipment are more effective than others in reducing crime and to what extent their effects differ.

To begin, we need to load two datasets:

* *militarization.RDS* – our primary panel dataset at the county level

* *categories.RDS* – a dataset that contains disaggregated information on the types of equipment transferred

Simply press `check` to run the chunk below and load the data.

```{r "18_1"}

mil <- readRDS("data/militarization.RDS")

cat <- readRDS("data/categories.RDS")

```

To begin, we will examine the composition of military aid in more detail by exploring the structure of the `cat` dataset. This dataset provides a granular view of the types of equipment transferred to law enforcement agencies and is essential for understanding how the broader aid categories—such as weapons or vehicles—are constructed.

**Task:** Use the `head()` function to display the first six rows of the `cat` dataset and get a sense of its structure.

```{r "18_2"}
# Enter your code here.
nrow(cat)
```


The `cat` dataset includes information on 231,892 individual transactions carried out under the 1033 Program. Each row represents one specific delivery to a law enforcement agency and contains three key variables:

* The **total value** of the transferred item,

* The detailed **federal supply category**, and

* A corresponding **top-level category**, which groups each item into one of four broader classifications: weapons, vehicles, gear, or other equipment.

This dataset allows us to go beyond aggregate measures of military aid and investigate whether certain types of equipment are more strongly associated with crime reduction than others. In the next step, we will examine how the numerous detailed federal supply categories are mapped into these four top-level groups. Additionally, we will assess the relative frequency and value of each category to better understand their role within the overall structure of the 1033 Program.

To proceed, simply press `check` to run the next code chunk and generate a summary table.

```{r "18_3"}
cat_table <- cat |>
  mutate(
    category = factor(
      category,
      levels = c("weapons", "vehicles", "gears", "other equipment"),
      labels = c("Weapons", "Vehicles", "Gears", "Others")
    )
  )

summary_tab <- cat_table |>
  group_by(category, federal_supply_categorie) |>
  summarise(
    count = n(),
    avg   = mean(aid_value, na.rm = TRUE),
    .groups = "drop"
  )

group_summary <- cat_table |>
  group_by(category) |>
  summarise(
    federal_supply_categorie = "Total",
    count = n(),
    avg   = mean(aid_value, na.rm = TRUE),
    .groups = "drop"
  )

combined <- bind_rows(summary_tab, group_summary) |>
  arrange(
    category,
    federal_supply_categorie == "Total",
    federal_supply_categorie
  ) |>
  ungroup()

group_df <- combined |>
  count(category, name = "n")

group_info <- setNames(group_df$n, as.character(group_df$category))

combined |>
  select(-category) |>
  rename(
    sub_category  = federal_supply_categorie,
    `Average Value` = avg,
    `Freq.`         = count
  ) |>
  select(
    sub_category,
    `Average Value`,
    `Freq.`
  ) |>
  kbl(
    col.names = c(" ", "Average Value", "Freq."),
    digits    = 2,
    format    = "html"
  ) |>
  kable_styling(full_width = FALSE) |>
  pack_rows(
    index = group_info,
    bold  = TRUE
  )
```


The table presents the average transaction value and frequency of all federal supply categories, grouped into the four main aid types: *Weapons*, *Vehicles*, *Gear*, and *Other.*

The *Weapons* category is the smallest in terms of diversity, comprising only three subcategories. Among them, the most frequently delivered item—simply labeled "Weapons"—has the lowest average transaction value. This finding is consistent with expectations, as this subcategory primarily includes firearms such as pistols and rifles, which are frequently distributed in bulk through the 1033 Program.

Within the *Vehicles* category, the subcategory "Motor vehicles, cycles, trailers" dominates, accounting for more than half of all transactions. Its high average value—$67,076.26 per delivery—likely reflects the inclusion of costly military vehicles such as MRAPs (Mine-Resistant Ambush Protected vehicles). While other vehicle-related subcategories exist, such as "Aircraft/airframe structure components" (750 deliveries), they are far less common compared to the 10,425 transactions involving ground vehicles.

In the *Gear* category, the majority of transfers consist of clothing, communications equipment, and fire control devices. Among these, clothing is the most visibly militarized due to its distinctive style and design. In contrast, the remaining items could plausibly serve both civilian and law enforcement purposes, making the degree of militarization within this category more ambiguous.

The *Other* category is the most heterogeneous and includes a broad assortment of items not directly associated with policing or military operations. This class includes everyday goods such as toiletries, agricultural machinery, and hand tools—the latter being the most frequently delivered item within this group. Functionally, this category serves as a residual classification for all items that do not clearly fall into the other three.

Having now gained a clearer understanding of the composition and distribution of aid types, we turn to estimating their distinct effects on crime. Specifically, we replicate the analysis from Exercise 4.1, but instead of using total military aid as the independent variable, we run separate regressions for each aid category to assess whether some types are more effective than others in reducing crime.

**Note:** The following code chunk runs several regressions and may take a few moments to execute. Simply `press` check to begin the analysis.

```{r "18_4"}
expvars_4 <- c("Total Aid" = "lag_log_aid_value", "Weapons" = "lag_log_weapons_value", "Vehicles" = "lag_log_vehicles_value", "Gears" = "lag_log_gears_value", "Other Equipment" = "lag_log_other_equipment_value")

depvars_4 <- c("Crime" = "crime_rate", "Murder" = "murder_rate", "Robbery" = "robbery_rate", "Assault" = "assault_rate", "Burglary" = "burglary_rate", "Larceny" = "larceny_rate", "Vehicle Theft" = "vehicle_theft_rate")

results_4 <- list()

for (u in c(1:length(expvars_4))) {
  
  list_temp <- list()
  
  for (i in c(1:length(depvars_4))) {
    
    formula <- as.formula(paste(depvars_4[[i]], " ~ log_median_income + poverty_rate + unempl_rate + log_population + share_male +     share_black + share_15_19_years + share_20_24_years + share_25_29_years + share_30_34_years | county + state_year_interaction |     ", expvars_4[[u]], " ~ milex_iv"))
    model <- feols(formula, data = mil, cluster = ~ state, ssc = ssc(adj=FALSE, cluster.adj = FALSE, t.df = "conventional"))
    list_temp[[names(depvars_4[i])]] <- model
  }
  
  results_4[[names(expvars_4[u])]] <- list_temp
}



modelsummary(
  models = results_4,
  shape = "rbind",
  coef_map = c("fit_lag_log_aid_value" = "lag_log_aid_value", 
               "fit_lag_log_weapons_value" = "lag_log_weapons_value",
               "fit_lag_log_vehicles_value" = "lag_log_vehicles_value",
               "fit_lag_log_gears_value" = "lag_log_gears_value",
               "fit_lag_log_other_equipment_value" = "lag_log_other_equipment_value"),
  gof_map = list(
    list(raw   = "nobs", clean = "Num. Obs.", fmt   = function(x) x),
    list(raw   = "kp_f_stat", clean = "KP F-Statistic", fmt   = function(x) x)
  ),
  stars = TRUE,
  output = "kableExtra"
)|>
  kable_classic(full_width = TRUE) |>
  kable_styling(font_size = 15)

```


The regression table indicates that the *Weapons* category is associated with positive coefficients across all crime types. However, statistical significance is only observed for vehicle theft and the overall crime rate. Notably, weapons are the only category for which the coefficients are consistently positive—though mostly not statistically significant. These results should be interpreted with caution: as shown in Exercise 2.3, the Weapons category represents by far the smallest share of total aid volume. This limited relevance is reflected in a low Kleibergen-Paap (KP) F-statistic of 4.377, which falls below conventional thresholds (typically >10), indicating potential issues with weak instrument bias. In comparison, the other categories have KP F-statistics that are much higher than the usual limits, indicating that the instrument is strong enough in those analysis. Because the instrumental variable remains the same (`milex_iv`) in all regressions the weapons category might be special and would need a other instrumental variable to address the endogeneity problem. Consequently, the reliability of the estimates of the weapons category is limited.

The *Other* category is showing the most negative effects on crime rates but also the highest standard errors. Interestingly, this category consists almost exclusively of non-lethal and non-military items, implying that the crime-reducing potential of aid may not stem solely from tactical capability but also from operational support functions.


Quiz: How can we interpret the coefficient of the lagged, logarithmized other equipment aid in the regression on crime rate if we assume a causal relationship?

(1) 10% more aid leads to approximately 7 less crimes per 100,000 population.
(2) 10$ more aid leads to approximately 7 more crimes per 100,000 population.
(3) 10% more aid leads to approximately 70 less crimes per 100,000 population.
(4) 10$ more aid leads to approximately 7% less crimes per 100,000 population.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("coeffiecient_interpretation_4")
```

<br/>

While the *Other* category is broad and somewhat heterogeneous, it includes office furniture, IT equipment, and administrative tools. Such items may enhance police departments’ organizational efficiency—for example, by improving coordination or freeing up time for proactive patrols. These findings are consistent with Mastrobuoni (2020), who argues that digital infrastructure and IT innovation in law enforcement can lead to reductions in crime. This conclusion is consistent with our findings from exercise 4.3 where we find a negative effect of military aid on the number of officers and civilian employees in law enforcement agencies.

The *Vehicles* and *Gear* categories show similar patterns in both the magnitude and significance of their effects. Though slightly smaller than the coefficients observed for *Total Aid* and *Others*, they mirror the same general trends. Both categories include highly visible forms of equipment—vehicles by their nature and gear, particularly clothing, which is often military in appearance. Such items may influence public perceptions of police authority and presence, thereby reinforcing a visual deterrence effect, as discussed in Bell (1982).

These differentiated effects highlight the importance of disaggregating aid types when evaluating the effectiveness of police militarization. Aggregated measures may obscure important variation in how different forms of equipment contribute to crime reduction—or fail to do so.

In conclusion, this analysis demonstrates that the effectiveness of military aid depends heavily on the type of equipment transferred to law enforcement agencies. While weapons show weak and positive effects on crime rates—combined with methodological concerns due to a weak instrument—the Other category, consisting primarily of non-tactical and non-lethal equipment, exhibits the strongest crime-reducing effects. The evidence suggests that administrative and technical support may contribute more effectively to improved policing than overtly militarized gear. Overall, the findings highlight the importance of a nuanced evaluation of the 1033 Program: rather than drawing broad conclusions about police militarization, it is crucial to consider what kind of aid is being distributed and how it is being used in practice.

<br/>


## Exercise 5 -- Conclusion

This problem set has explored the effects of police militarization in the United States through a detailed, multi-stage empirical analysis of the 1033 Program—a federal initiative that transfers surplus military equipment to local law enforcement agencies. Using a combination of panel data, fixed effects models, and instrumental variable techniques, we examined how military aid affects crime rates, arrest behavior, police operations, and citizen–police relations.

Our analysis began by establishing the policy background of the 1033 Program, highlighting its expansion in the post-9/11 era and its increasing visibility in the context of police–civilian confrontations. This context provided both the empirical motivation and theoretical grounding for the questions pursued in subsequent sections.


### Crime and Arrest Effects

The first part of the analysis (Exercises 2 and 3) showed that basic OLS models indicated an unexpected positive link between military aid and crime rates, probably due to endogeneity—meaning that areas with high crime tend to get more aid. By introducing fixed effects and instrumenting military aid with lagged national defense expenditures that interacted with historical aid receipt probabilities, we identified a negative and statistically significant effect of military aid on total crime, particularly for street-level offenses such as robbery, larceny, and vehicle theft. These findings are robust across different specifications and hold even when controlling for a wide range of demographic and economic covariates.

However, when examining arrest rates in Exercises 4.1 and 4.2, we found no significant evidence that militarization improves the rate at which crimes lead to arrests. Absolute arrest rates declined in line with falling crime, but relative arrest rates—arrests per reported crime—remained unchanged or declined slightly, suggesting that military aid does not enhance enforcement effectiveness per se.

This discrepancy between decreasing crime and unchanged law enforcement ability suggests that seeing military equipment can discourage people from committing crimes, not because the chance of getting caught has gone up, but because they feel the risks of dealing with armed police have increased.


### Police Behavior, Public Interaction and Equipment Types

In Exercise 4.3, we shifted focus to police organizational behavior and public interaction by analyzing agency-level data. The results indicated that military aid is associated with slight reductions in staffing levels for both sworn officers and civilian employees, suggesting a substitution of labor by capital. This supports the interpretation that military equipment may improve the productivity of existing staff, reducing the marginal need for new hires without altering the overall officer-to-staff balance.

Importantly, we found no significant effects of military aid on service call volume, officer injuries, or use of deadly force. These findings further support the idea that militarization does not lead to systematically more aggressive or interventionist policing.

We also examined the number of citizen complaints—a key proxy for public dissatisfaction or perceptions of misconduct. Here, the coefficient was small, negative, and statistically insignificant. However, this result should be interpreted with caution due to limited data availability and low instrument strength. As scholars such as Harris et al. (2017) note, militarization may simultaneously increase aggressive behavior and suppress reporting due to fear or intimidation, making interpretation difficult.

In Exercise 4.4, we added a more nuanced dimension to the analysis by disaggregating military aid into four supply categories: weapons, vehicles, gear, and other. The results revealed that not all categories equally drive the effect of military aid on crime. While weapons aid showed weak and even slightly positive associations with crime, the “Other” category—which includes non-lethal, often civilian-type equipment such as IT or administrative tools—had the strongest negative effect on crime rates. This suggests that organizational capacity improvements may play a bigger role than traditionally assumed. Gear and vehicles also showed moderate negative effects, aligning with the visual deterrence hypothesis. These findings highlight the importance of moving beyond aggregate measures of military aid to better understand the underlying mechanisms at work.


### Interpretation and Broader Implications

Taken together, our findings suggest that military aid under the 1033 Program can contribute to reductions in certain types of crime, but not by making police more effective at catching offenders. Instead, the reductions seem to result from a symbolic or psychological deterrent effect, where the presence of military-grade equipment changes offender behavior before a crime occurs.

While such deterrence may be effective in the short term, it raises important normative and practical questions. Relying on fear or intimidation as a crime control strategy risks undermining long-term community trust, especially in marginalized communities where policing is already contested. Moreover, the effectiveness of militarization may decline over time as communities adapt to its presence, and it may have unintended chilling effects on public willingness to engage with law enforcement.


### Methodological Reflection

From a methodological perspective, the use of fixed effects and instrumental variable regression allowed us to mitigate key threats to causal inference, particularly unobserved heterogeneity. The inclusion of lagged national defense expenditure interacted with county-level aid propensity proved to be a strong and credible instrument in most models, though its performance was weaker in smaller sub-samples (e.g., citizen complaints).

Still, some limitations remain. Our analyses rely on administrative data, which may underreport certain outcomes (e.g., misconduct or abuse) and do not capture qualitative aspects of police–community relations. Moreover, our outcome variables reflect short- to medium-term impacts; the long-term consequences of militarization—for example, changes in community cohesion, democratic oversight, or officer behavior over time—remain outside the scope of this analysis.


### Policy Outlook and Future Research

The results suggest that military aid may play a limited but real role in reducing certain types of crime, primarily through deterrence. However, the absence of positive effects on police effectiveness or public trust raises concerns about the broader societal implications of relying on militarized policing as a crime prevention strategy.

Policy discussions about the 1033 Program—and police militarization more broadly—should go beyond narrow metrics of crime reduction. Future research should:

* Examine heterogeneous effects across communities, particularly those with histories of police overreach.

* Investigate long-term behavioral impacts on both officers and civilians.

* Integrate survey data and qualitative methods to assess legitimacy, trust, and accountability.

In sum, this problem set has shown that while military aid may deter crime, it does not solve the deeper structural and institutional challenges facing American policing. Understanding the limits and trade-offs of militarization is essential for building more effective, equitable, and trusted systems of public safety.

<br/>

## Exercise 6 -- References


### Bibliography


ACLU. (2014). *War comes home: The excessive militarization of American policing*.

Angrist, J. D., & Pischke, J. S. (2009). *Mostly harmless econometrics: An empiricist's companion*. Princeton University Press.

Arellano, M. (1987). Computing robust standard errors for within-groups estimators. *Oxford Bulletin of Economics and Statistics, 49*(4), 431–434. https://doi.org/10.1111/j.1468-0084.1987.mp49004006.x

Arellano, M. (2003). *Panel data econometrics*. Oxford University Press.

Baltagi, B. H. (2021). *Econometric analysis of panel data*. Springer.

Balko, R. (2013). *Rise of the warrior cop: The militarization of America’s police forces*. PublicAffairs.

Bell, D. J. (1982). Police uniforms, attitudes, and citizens. *Journal of criminal justice*, 10(1), 45-55.

Bell, M. C. (2017). Police reform and the dismantling of legal estrangement. *Yale Law Journal, 126*(7), 2054–2150.

Bove, V., & Gavrilova, E. (2017). Police officer on the frontline or a soldier? The effect of police militarization on crime. *American Economic Journal: Economic Policy, 9*(3), 1–18.

Butts, J. A., Roman, C. G., Bostwick, L., & Porter, J. R. (2015). Cure Violence: A public health model to reduce gun violence. *Annual Review of Public Health, 36*, 39–53.

Cameron, A. C., & Miller, D. L. (2015). A practitioner’s guide to cluster-robust inference. *Journal of Human Resources, 50*(2), 317–372. https://doi.org/10.3368/jhr.50.2.317

Delehanty, C., Mewhirter, J., Welch, R., & Wilks, J. (2017). Militarization and police violence: The case of the 1033 program. *Research & Politics, 4*(2), 1–7.

Defense Logistics Agency. (2020). *1033 program overview*.

Department of Justice. (2015). *Investigation of the Ferguson Police Department*.

Glaeser, E. L., & Sacerdote, B. (1999). Why is there more crime in cities? *Journal of Political Economy, 107*(S6), S225–S258.

Harris, M. C., Park, J., Bruce, D. J., & Murray, M. N. (2017). Peacekeeping force: Effects of providing tactical equipment to local law enforcement. *American Economic Journal: Economic Policy, 9*(3), 291–313.

Hausman, J., Stock, J. H., & Yogo, M. (2005). Asymptotic properties of the Hahn–Hausman test for weak-instruments. *Economics Letters, 89*(3), 333–342.

Heaton, P. (2010). *Hidden in plain sight: What cost-of-crime research can tell us about investing in police*. RAND Center on Quality Policing.

Kleibergen, F., & Paap, R. (2006). Generalized reduced rank tests using the singular value decomposition. *Journal of Econometrics, 133*(1), 97–126. https://doi.org/10.1016/j.jeconom.2005.02.017

Kraska, P. B. (2007). Militarization and policing—Its relevance to 21st century police. *Policing: A Journal of Policy and Practice, 1*(4), 501–513.

Leong, C. E. (2014). A review of research on crime displacement theory. *International Journal of Business and Economics Research, 3*(6-1), 22–30. https://doi.org/10.11648/j.ijber.s.2014030601.14

MacKinnon, J. G., & Webb, M. D. (2017). Wild bootstrap inference for wildly different cluster sizes. *Journal of Applied Econometrics, 32*(2), 233–254.

Mastrobuoni, G. (2020). Crime is terribly revealing: Information technology and police productivity. *The Review of Economic Studies, 87*(6), 2727-2753.

Moulton, B. R. (1986). Random group effects and the precision of regression estimates. *Journal of Econometrics, 32*(3), 385–397. https://doi.org/10.1016/0304-4076(86)90021-7

Mummolo, J. (2018). Militarization fails to enhance police safety or reduce crime but may harm police reputation. *Proceedings of the National Academy of Sciences, 115*(37), 9181–9186.

Nunn, N., & Qian, N. (2014). US food aid and civil conflict. *American Economic Review, 104*(6), 1630–1666.

Osgood, D. W., & Chambers, J. M. (2000). Social disorganization outside the metropolis: An analysis of rural youth violence. *Criminology, 38*(1), 81–116.

Radil, S. M., & Dehlin, E. (2020). Geographies of US police militarization and the role of the 1033 program. *The Professional Geographer, 72*(2), 196–208.

Roodman, D., MacKinnon, J. G., Nielsen, M. Ø., & Webb, M. D. (2019). Fast and wild: Bootstrap inference in Stata using boottest. *The Stata Journal, 19*(1), 4–60.

Stock, J. H., & Watson, M. W. (2015). *Introduction to econometrics*. Pearson.

Stock, J. H., & Watson, M. W. (2020). *Introduction to econometrics* (4th ed.). Pearson.

Welch, R. (2017). Police militarization and the use of lethal force. *Political Research Quarterly, 70*(3), 618–631.

Wooldridge, J. M. (2010). *Econometric analysis of cross section and panel data*. MIT Press.

Wright, R. T., & Decker, S. H. (1994). *Burglars on the job: Streetlife and residential break-ins*. Northeastern University Press.

Zeileis, A. (2004). Econometric computing with HC and HAC covariance matrix estimators. *Journal of Statistical Software, 11*(10), 1–17. https://doi.org/10.18637/jss.v011.i10

</br>

### R-Packages

Arel-Bundock, V. (2022). modelsummary: Data and model summaries in R. *Journal of Statistical Software, 103*(1), 1–23. https://doi.org/10.18637/jss.v103.i01

Berge, L. (2018). Efficient estimation of maximum likelihood models with multiple fixed-effects: The R package FENmlm. *CREA Discussion Papers*.

Fischer, M., & Roodman, D. (2021). *fwildclusterboot: Fast Wild Cluster Bootstrap Inference for Linear Regression Models* (R package version). https://CRAN.R-project.org/package=fwildclusterboot

Revelle, W. (2024). *psych: Procedures for Psychological, Psychometric, and Personality Research* (R package version 2.4.3). Northwestern University, Evanston, Illinois. https://CRAN.R-project.org/package=psych

Wickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). *dplyr: A grammar of data manipulation* (R package version 1.1.4). https://CRAN.R-project.org/package=dplyr

Wickham, H., Vaughan, D., & Girlich, M. (2024). *tidyr: Tidy messy data* (R package version 1.3.1). https://CRAN.R-project.org/package=tidyr

Wickham, H. (2016). *ggplot2: Elegant graphics for data analysis*. Springer-Verlag New York.

Zhu, H. (2024). *kableExtra: Construct complex table with 'kable' and pipe syntax* (R package version 1.4.0). https://CRAN.R-project.org/package=kableExtra


<br/>
